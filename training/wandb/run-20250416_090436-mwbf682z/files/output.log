  0%|                                                    | 0/80 [00:00<?, ?it/s]
0 25113 0.5395911335945129
1 25113 0.644398033618927
2 25113 0.4711369574069977
3 25113 0.4425882399082184
4 25113 0.41731148958206177
5 25113 0.4928896129131317
6 25113 0.4739498198032379
7 25113 0.3786724805831909
8 25113 0.3257194459438324
9 25113 0.3792213499546051
10 25113 0.4010239541530609
11 25113 0.34039685130119324
12 25113 0.34967148303985596
13 25113 0.3605932891368866
14 25113 0.36861783266067505
15 25113 0.45678389072418213
16 25113 0.38671576976776123
17 25113 0.5525136590003967
18 25113 0.3701512813568115
19 25113 0.47509142756462097
20 25113 0.5375492572784424
21 25113 0.4194384515285492
22 25113 0.347721129655838
23 25113 0.3400663137435913
24 25113 0.373751163482666
25 25113 0.2805463373661041
26 25113 0.37291017174720764
27 25113 0.3908648192882538
28 25113 0.35591384768486023
29 25113 0.3469695746898651
30 25113 0.40560054779052734
31 25113 0.43283817172050476
32 25113 0.4574199914932251
33 25113 0.348662793636322
34 25113 0.4385090470314026
35 25113 0.401020884513855
36 25113 0.39899206161499023
37 25113 0.5013864636421204
38 25113 0.5104236602783203
39 25113 0.3818552792072296
40 25113 0.4161931574344635
41 25113 0.3453516364097595
42 25113 0.4777367413043976
43 25113 0.4600529074668884
44 25113 0.4238578975200653
45 25113 0.4115791618824005
46 25113 0.4398435056209564
47 25113 0.41198673844337463
48 25113 0.41690248250961304
49 25113 0.417334645986557
50 25113 0.47802379727363586
51 25113 0.44800320267677307
52 25113 0.5382563471794128
53 25113 0.46756377816200256
54 25113 0.47765347361564636
55 25113 0.4831659495830536
56 25113 0.3468293845653534
57 25113 0.4695754647254944
58 25113 0.4902171790599823
59 25113 0.4157762825489044
60 25113 0.429580956697464
61 25113 0.35785001516342163
62 25113 0.31816405057907104
63 25113 0.3237067759037018
64 25113 0.3331475257873535
65 25113 0.32618969678878784
66 25113 0.3852814733982086
67 25113 0.37518587708473206
68 25113 0.38714203238487244
69 25113 0.38417840003967285
70 25113 0.4733758568763733
71 25113 0.3079610764980316
72 25113 0.4806556701660156
73 25113 0.3935151696205139
74 25113 0.3885520100593567
75 25113 0.42900896072387695
76 25113 0.5122556090354919
77 25113 0.4625515341758728
78 25113 0.44997331500053406
79 25113 0.44835904240608215
80 25113 0.3896806836128235
81 25113 0.3497348725795746
82 25113 0.48091232776641846
83 25113 0.3775130808353424
84 25113 0.42497679591178894
85 25113 0.45903509855270386
86 25113 0.46313950419425964
87 25113 0.4109152853488922
88 25113 0.555560290813446
89 25113 0.32752475142478943
90 25113 0.3014630973339081
91 25113 0.31270360946655273
92 25113 0.3121034502983093
93 25113 0.365302711725235
94 25113 0.42474719882011414
95 25113 0.33789411187171936
96 25113 0.31950730085372925
97 25113 0.3201475441455841
98 25113 0.3011198937892914
99 25113 0.33396875858306885
100 25113 0.25437867641448975
101 25113 0.43749672174453735
102 25113 0.31897836923599243
103 25113 0.3643818497657776
104 25113 0.4571225643157959
105 25113 0.4123689830303192
106 25113 0.4079064428806305
107 25113 0.3862147033214569
108 25113 0.4504070580005646
109 25113 0.3274262845516205
110 25113 0.4656071066856384
111 25113 0.4905402362346649
112 25113 0.33043012022972107
113 25113 0.4045298993587494
114 25113 0.35036954283714294
115 25113 0.41181713342666626
116 25113 0.448973149061203
117 25113 0.33532020449638367
118 25113 0.4698615074157715
119 25113 0.4343460202217102
120 25113 0.3498201370239258
121 25113 0.40016239881515503
122 25113 0.41811418533325195
123 25113 0.24065491557121277
124 25113 0.27395254373550415
125 25113 0.2861916124820709
126 25113 0.266064316034317
127 25113 0.34037086367607117
128 25113 0.3375394344329834
129 25113 0.3028959035873413
130 25113 0.2843019664287567
131 25113 0.3196602463722229
132 25113 0.27040237188339233
133 25113 0.2880052924156189
134 25113 0.3782225549221039
135 25113 0.30982643365859985
136 25113 0.3575887680053711
137 25113 0.37359580397605896
138 25113 0.43744754791259766
139 25113 0.3140341639518738
140 25113 0.3767574429512024
141 25113 0.3308282792568207
142 25113 0.31798702478408813
143 25113 0.31163495779037476
144 25113 0.3166391849517822
145 25113 0.37674158811569214
146 25113 0.3112817406654358
147 25113 0.359245240688324
148 25113 0.3750316798686981
149 25113 0.33910810947418213
150 25113 0.39047300815582275
151 25113 0.3649371564388275
152 25113 0.3306538462638855
153 25113 0.2772684693336487
154 25113 0.34405940771102905
155 25113 0.3444330394268036
156 25113 0.37657833099365234
157 25113 0.4285373389720917
158 25113 0.4137422740459442
159 25113 0.31637540459632874
160 25113 0.36839231848716736
161 25113 0.29729345440864563
162 25113 0.4216516613960266
163 25113 0.48905399441719055
164 25113 0.36750128865242004
165 25113 0.3166002035140991
166 25113 0.42912164330482483
167 25113 0.4322549104690552
168 25113 0.3938899040222168
169 25113 0.4487466514110565
170 25113 0.4107554852962494
171 25113 0.3451481759548187
172 25113 0.36252132058143616
173 25113 0.2652440369129181
174 25113 0.27770915627479553
175 25113 0.23341341316699982
176 25113 0.36248213052749634
177 25113 0.30540385842323303
178 25113 0.3452378213405609
179 25113 0.2875484228134155
180 25113 0.3659881353378296
181 25113 0.2852664887905121
182 25113 0.3986221253871918
183 25113 0.3212251365184784
184 25113 0.328722208738327
185 25113 0.2799806296825409
186 25113 0.30065158009529114
187 25113 0.3844444751739502
188 25113 0.33757317066192627
189 25113 0.2510824203491211
190 25113 0.319721519947052
191 25113 0.38302871584892273
192 25113 0.35449573397636414
193 25113 0.4076738655567169
194 25113 0.3426308333873749
195 25113 0.3016648590564728
196 25113 0.5130059719085693
train - Global loss: 0.383682    Embedding norm: 1.0000   Triplets (all/active): 127.5/14916.6
Pos dist (min/mean/max): 0.7990/1.2541/1.6506   Neg dist (min/mean/max): 0.8900/1.4058/1.7636
0 1011 0.395534873008728
1 1011 0.3181956112384796
2 1011 0.28405502438545227
3 1011 0.39263415336608887
4 1011 0.5177996158599854
5 1011 0.2683478891849518
6 1011 0.40190139412879944
7 1011 0.6565311551094055
val - Global loss: 0.404375    Embedding norm: 1.0000   Triplets (all/active): 126.4/14408.9
Pos dist (min/mean/max): 0.2305/0.7201/1.2381   Neg dist (min/mean/max): 0.2450/0.9350/1.6286
0 25113 0.3339338004589081
1 25113 0.3678245544433594
2 25113 0.3972819447517395
3 25113 0.2811530530452728
4 25113 0.2956237196922302
5 25113 0.4154682457447052
6 25113 0.4523281753063202
7 25113 0.36085397005081177
8 25113 0.3078679144382477
9 25113 0.31624671816825867
10 25113 0.3087501525878906
11 25113 0.2613297402858734
12 25113 0.2948208153247833
13 25113 0.27940937876701355
14 25113 0.4071769118309021
15 25113 0.4025355875492096
16 25113 0.35880181193351746
17 25113 0.468860924243927
18 25113 0.3480326533317566
19 25113 0.36029937863349915
20 25113 0.4142944812774658
21 25113 0.3928650915622711
22 25113 0.3483136296272278
23 25113 0.3336440324783325
24 25113 0.25572913885116577
25 25113 0.2941000759601593
26 25113 0.28532716631889343
27 25113 0.21334058046340942
28 25113 0.3132362365722656
29 25113 0.31795769929885864
30 25113 0.36502188444137573
31 25113 0.39139336347579956
32 25113 0.33430859446525574
33 25113 0.26867610216140747
34 25113 0.3128371834754944
35 25113 0.31475409865379333
36 25113 0.3612903952598572
37 25113 0.4298432469367981
38 25113 0.3848574161529541
39 25113 0.29482725262641907
40 25113 0.2582930326461792
41 25113 0.3514314293861389
42 25113 0.3878801465034485
43 25113 0.37379026412963867
44 25113 0.36985141038894653
45 25113 0.3430478274822235
46 25113 0.3154352307319641
47 25113 0.32681137323379517
48 25113 0.267862468957901
49 25113 0.36703962087631226
50 25113 0.4294397830963135
51 25113 0.3241892158985138
52 25113 0.32297131419181824
53 25113 0.31547605991363525
54 25113 0.3164398670196533
55 25113 nan
56 25113 nan
57 25113 nan
58 25113 nan
59 25113 nan
60 25113 nan
61 25113 nan
62 25113 nan
63 25113 nan
64 25113 nan
65 25113 nan
66 25113 nan
67 25113 nan
68 25113 nan
Traceback (most recent call last):
  File "/code/RLoc/training/train.py", line 31, in <module>
    do_train(params, debug=args.debug, device=device)
  File "/code/RLoc/training/trainer.py", line 393, in do_train
    outputs = model(inputs, n_pos, n_neg)
  File "/usr/local/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1130, in _call_impl
    return forward_call(*input, **kwargs)
  File "/usr/local/lib/python3.9/site-packages/torch/nn/parallel/data_parallel.py", line 168, in forward
    outputs = self.parallel_apply(replicas, inputs, kwargs)
  File "/usr/local/lib/python3.9/site-packages/torch/nn/parallel/data_parallel.py", line 178, in parallel_apply
    return parallel_apply(replicas, inputs, kwargs, self.device_ids[:len(replicas)])
  File "/usr/local/lib/python3.9/site-packages/torch/nn/parallel/parallel_apply.py", line 78, in parallel_apply
    thread.join()
  File "/usr/local/lib/python3.9/threading.py", line 1053, in join
    self._wait_for_tstate_lock()
  File "/usr/local/lib/python3.9/threading.py", line 1073, in _wait_for_tstate_lock
    if lock.acquire(block, timeout):
KeyboardInterrupt
