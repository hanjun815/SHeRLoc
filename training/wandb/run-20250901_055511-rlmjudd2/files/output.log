  0%|                                                    | 0/20 [00:00<?, ?it/s]
0 45360 0.3442641794681549
1 45360 0.3773791491985321
2 45360 0.3540765643119812
3 45360 0.35672295093536377
4 45360 0.3383009731769562
5 45360 0.3601621091365814
6 45360 0.29331016540527344
7 45360 0.3548039197921753
8 45360 0.39884287118911743
9 45360 0.3856639862060547
10 45360 0.4130326509475708
11 45360 0.38409075140953064
12 45360 0.33730560541152954
13 45360 0.39919233322143555
14 45360 0.34386128187179565
15 45360 0.351077675819397
16 45360 0.37523773312568665
17 45360 0.4046720862388611
18 45360 0.36397895216941833
19 45360 0.3551252484321594
20 45360 0.37858352065086365
21 45360 0.4208777844905853
22 45360 0.35091906785964966
23 45360 0.34087175130844116
24 45360 0.3400547504425049
25 45360 0.3782975673675537
26 45360 0.4119812548160553
27 45360 0.37188059091567993
28 45360 0.32147282361984253
29 45360 0.3558426797389984
30 45360 0.2770472466945648
31 45360 0.27520516514778137
32 45360 0.31943953037261963
33 45360 0.34388771653175354
34 45360 0.3273824453353882
35 45360 0.32460182905197144
36 45360 0.35665902495384216
37 45360 0.31090399622917175
38 45360 0.3570000231266022
39 45360 0.32391637563705444
40 45360 0.32798099517822266
41 45360 0.3130517899990082
42 45360 0.34042587876319885
43 45360 0.29512715339660645
44 45360 0.2887716293334961
45 45360 0.2595534324645996
46 45360 0.27202850580215454
47 45360 0.27357804775238037
48 45360 0.38696420192718506
49 45360 0.29137229919433594
50 45360 0.3226270377635956
51 45360 0.3093007802963257
52 45360 0.3007689118385315
53 45360 0.28830233216285706
54 45360 0.2639763653278351
55 45360 0.3409815728664398
56 45360 0.2535386085510254
57 45360 0.3011821210384369
58 45360 0.2964790463447571
59 45360 0.2745214104652405
60 45360 0.3046573996543884
61 45360 0.2857721745967865
62 45360 0.24125421047210693
63 45360 0.25036656856536865
64 45360 0.3056321144104004
65 45360 0.3550640940666199
66 45360 0.2875921130180359
67 45360 0.28236696124076843
68 45360 0.2690538167953491
69 45360 0.26643314957618713
70 45360 0.29584264755249023
71 45360 0.28971776366233826
72 45360 0.24562610685825348
73 45360 0.22892184555530548
74 45360 0.3020479083061218
75 45360 0.3061258792877197
76 45360 0.23450005054473877
77 45360 0.26853686571121216
78 45360 0.3346373736858368
79 45360 0.2711113393306732
80 45360 0.23387597501277924
81 45360 0.2459060251712799
82 45360 0.21902523934841156
83 45360 0.2545851171016693
84 45360 0.27246806025505066
85 45360 0.22579702734947205
86 45360 0.22724731266498566
87 45360 0.25988447666168213
88 45360 0.22759829461574554
89 45360 0.24344724416732788
90 45360 0.24818606674671173
91 45360 0.30825144052505493
92 45360 0.2480984926223755
93 45360 0.25657424330711365
94 45360 0.2048962563276291
95 45360 0.22238342463970184
96 45360 0.23383444547653198
97 45360 0.20801876485347748
98 45360 0.25313517451286316
99 45360 0.4249767065048218
100 45360 0.315928190946579
101 45360 0.34595394134521484
102 45360 0.3820229470729828
103 45360 0.3847489356994629
104 45360 0.37471118569374084
105 45360 0.3499807119369507
106 45360 0.24173389375209808
107 45360 0.32052722573280334
108 45360 0.2894938588142395
109 45360 0.2643173933029175
110 45360 0.24341027438640594
111 45360 0.2254227250814438
112 45360 0.23872075974941254
113 45360 0.2459031641483307
114 45360 0.30447137355804443
115 45360 0.3335080146789551
116 45360 0.2891875207424164
117 45360 0.25294214487075806
118 45360 0.24978816509246826
119 45360 0.2742476463317871
120 45360 0.2980596721172333
121 45360 0.34670165181159973
122 45360 0.2657240927219391
123 45360 0.21043646335601807
124 45360 0.200885608792305
125 45360 0.23857615888118744
126 45360 0.21021242439746857
127 45360 0.2863091230392456
128 45360 0.27098459005355835
129 45360 0.24306929111480713
130 45360 0.2506869435310364
131 45360 0.2741599380970001
132 45360 0.2405083030462265
133 45360 0.26053401827812195
134 45360 0.24971887469291687
135 45360 0.31283023953437805
136 45360 0.20078861713409424
137 45360 0.20240086317062378
138 45360 0.21015727519989014
139 45360 0.246375173330307
140 45360 0.250610888004303
141 45360 0.2629583477973938
142 45360 0.23035648465156555
143 45360 0.2309521585702896
144 45360 0.19836647808551788
145 45360 0.20871582627296448
146 45360 0.20643675327301025
147 45360 0.24763979017734528
148 45360 0.254880428314209
149 45360 0.201592817902565
150 45360 0.20206423103809357
151 45360 0.21246998012065887
152 45360 0.23736955225467682
153 45360 0.22547970712184906
154 45360 0.1949664056301117
155 45360 0.23345807194709778
156 45360 0.18339425325393677
157 45360 0.18941685557365417
158 45360 0.2001679390668869
159 45360 0.1925654411315918
160 45360 0.22533634305000305
161 45360 0.21823805570602417
162 45360 0.21837010979652405
163 45360 0.20127210021018982
164 45360 0.2294476181268692
165 45360 0.1826048344373703
166 45360 0.1963295042514801
167 45360 0.18902869522571564
168 45360 0.22441968321800232
169 45360 0.29655206203460693
170 45360 0.20732684433460236
171 45360 0.21786384284496307
172 45360 0.2150755077600479
173 45360 0.21988800168037415
174 45360 0.17412716150283813
175 45360 0.16593116521835327
176 45360 0.1582377552986145
177 45360 0.24442797899246216
train - Global loss: 0.280429    Embedding norm: 1.0000   Triplets (all/active): 254.8/55037.0
Pos dist (min/mean/max): 0.7788/1.2603/1.5913   Neg dist (min/mean/max): 0.8743/1.4075/1.7692
0 1011 0.227961465716362
1 1011 0.2785671055316925
2 1011 0.25414809584617615
3 1011 0.23509939014911652
val - Global loss: 0.248944    Embedding norm: 1.0000   Triplets (all/active): 252.8/42600.2
Pos dist (min/mean/max): 0.7332/1.1755/1.5521   Neg dist (min/mean/max): 0.7582/1.3667/1.6447
0 45360 0.23234803974628448
1 45360 0.20902155339717865
2 45360 0.2113630473613739
3 45360 0.23280450701713562
Traceback (most recent call last):
  File "/code/RLoc/training/train.py", line 31, in <module>
    do_train(params, debug=args.debug, device=device)
  File "/code/RLoc/training/trainer.py", line 410, in do_train
    loss.backward()
  File "/usr/local/lib/python3.9/site-packages/torch/_tensor.py", line 396, in backward
    torch.autograd.backward(self, gradient, retain_graph, create_graph, inputs=inputs)
  File "/usr/local/lib/python3.9/site-packages/torch/autograd/__init__.py", line 173, in backward
    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
RuntimeError: CUDA out of memory. Tried to allocate 1.41 GiB (GPU 0; 23.68 GiB total capacity; 14.84 GiB already allocated; 651.88 MiB free; 16.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
