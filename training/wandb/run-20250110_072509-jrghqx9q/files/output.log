  0%|                                                                                | 0/60 [00:00<?, ?it/s]
0 18011 3.084892988204956
1 18011 1.879739761352539
2 18011 2.1826770305633545
3 18011 1.7391736507415771
4 18011 1.2720621824264526
5 18011 1.5873881578445435
6 18011 1.4890527725219727
7 18011 1.091066598892212
8 18011 1.0491278171539307
9 18011 0.851809561252594
10 18011 1.0532889366149902
11 18011 0.7990381717681885
12 18011 0.8079699277877808
13 18011 0.7152223587036133
14 18011 0.77216637134552
15 18011 0.9507513046264648
16 18011 1.2758835554122925
17 18011 1.122997760772705
18 18011 1.3505210876464844
19 18011 1.27685546875
20 18011 1.041459560394287
21 18011 0.9461355209350586
22 18011 1.5995700359344482
23 18011 1.4011070728302002
24 18011 1.0845537185668945
25 18011 1.0544695854187012
26 18011 0.2741661071777344
27 18011 1.1723374128341675
28 18011 1.0946519374847412
29 18011 0.6568925380706787
30 18011 0.5804141163825989
31 18011 0.2918418347835541
32 18011 0.28990161418914795
33 18011 0.530051052570343
34 18011 0.850070595741272
35 18011 0.41298016905784607
36 18011 0.1922057867050171
37 18011 0.25555217266082764
38 18011 0.24919700622558594
39 18011 0.6704025864601135
40 18011 0.521147608757019
41 18011 1.2292636632919312
42 18011 1.3032183647155762
43 18011 0.8237501382827759
44 18011 1.561360478401184
45 18011 1.6331167221069336
46 18011 1.5284721851348877
47 18011 1.8402785062789917
48 18011 2.103771448135376
49 18011 1.0377392768859863
50 18011 3.3966479301452637
51 18011 1.5222795009613037
52 18011 1.3311653137207031
53 18011 1.0867202281951904
54 18011 3.106839418411255
55 18011 1.1542561054229736
56 18011 0.8594151139259338
57 18011 0.5657472014427185
58 18011 2.0014705657958984
59 18011 1.080668330192566
60 18011 1.7537310123443604
61 18011 1.3974360227584839
62 18011 1.4264202117919922
63 18011 0.8034278154373169
64 18011 1.647651195526123
65 18011 0.20801639556884766
66 18011 7.179070949554443
67 18011 1.36709725856781
68 18011 0.36183106899261475
69 18011 0.8062214255332947
70 18011 0.8928186297416687
71 18011 1.4326812028884888
72 18011 3.1574044227600098
73 18011 1.3520970344543457
74 18011 1.5369431972503662
75 18011 1.7734581232070923
76 18011 1.3780956268310547
77 18011 3.588554859161377
78 18011 2.2693142890930176
79 18011 1.061611533164978
80 18011 1.2438061237335205
81 18011 1.3327550888061523
82 18011 1.0014111995697021
83 18011 0.7206385731697083
84 18011 1.167065143585205
85 18011 0.7297768592834473
86 18011 1.2307844161987305
87 18011 0.9603450894355774
88 18011 1.7607873678207397
89 18011 1.1203515529632568
90 18011 1.6370265483856201
91 18011 1.431685209274292
92 18011 1.4376351833343506
93 18011 1.4158356189727783
94 18011 1.6893525123596191
95 18011 1.364445686340332
96 18011 1.3973184823989868
97 18011 1.4336389303207397
98 18011 1.8616633415222168
99 18011 1.4177851676940918
100 18011 1.267712116241455
101 18011 0.9682841300964355
102 18011 0.5038156509399414
103 18011 3.896864414215088
104 18011 1.5648754835128784
105 18011 1.579728126525879
106 18011 1.8485443592071533
107 18011 2.241340160369873
108 18011 2.004650115966797
109 18011 1.737523078918457
110 18011 1.819270133972168
111 18011 2.1329236030578613
112 18011 1.1164019107818604
113 18011 1.7319145202636719
114 18011 2.365070343017578
115 18011 1.1662054061889648
116 18011 1.4923672676086426
117 18011 1.295076847076416
118 18011 1.7256112098693848
119 18011 1.5122085809707642
120 18011 1.0737080574035645
121 18011 1.650866150856018
122 18011 1.8856916427612305
123 18011 2.515501022338867
124 18011 3.1827268600463867
125 18011 2.885573148727417
126 18011 1.522984504699707
127 18011 1.406010627746582
128 18011 2.674981117248535
129 18011 6.01475191116333
130 18011 0.9563860893249512
131 18011 1.330418586730957
132 18011 1.579958200454712
133 18011 2.0014712810516357
134 18011 1.166050910949707
135 18011 1.1153278350830078
136 18011 0.7365565299987793
137 18011 1.6697349548339844
138 18011 1.6913093328475952
139 18011 1.1575448513031006
140 18011 8.109576225280762
train - Global loss: 1.508741    Embedding norm: 17.6300   Triplets (all/active): 127.7/50.0
Pos dist (min/mean/max): 1.1810/2.4905/8.2684   Neg dist (min/mean/max): 4.9239/8.2788/12.6278
0 9318 1.5654956102371216
1 9318 1.7284095287322998
2 9318 1.4145983457565308
3 9318 1.8740208148956299
4 9318 1.554663896560669
5 9318 1.228554368019104
6 9318 1.409425973892212
7 9318 1.6995134353637695
8 9318 1.696590781211853
9 9318 1.458874225616455
10 9318 1.7497975826263428
11 9318 1.6567912101745605
12 9318 1.3869763612747192
13 9318 1.2934021949768066
14 9318 1.7213122844696045
15 9318 1.406102180480957
16 9318 1.5304720401763916
17 9318 1.6618297100067139
18 9318 2.2097678184509277
19 9318 1.515582799911499
20 9318 1.5613045692443848
21 9318 1.7570545673370361
22 9318 1.5934051275253296
23 9318 1.5874683856964111
24 9318 1.5445241928100586
25 9318 1.5963644981384277
26 9318 1.3030033111572266
27 9318 1.3945115804672241
28 9318 1.487760305404663
29 9318 1.716459035873413
30 9318 1.3565468788146973
31 9318 1.471541404724121
32 9318 1.5608203411102295
33 9318 1.6558200120925903
34 9318 1.6626904010772705
35 9318 1.349676489830017
36 9318 1.5558878183364868
37 9318 1.6323399543762207
38 9318 1.575477123260498
39 9318 1.8405706882476807
40 9318 1.512657880783081
41 9318 1.220513939857483
42 9318 1.5799671411514282
43 9318 1.7933964729309082
44 9318 1.617258906364441
45 9318 1.5367546081542969
46 9318 1.7531431913375854
47 9318 1.3648262023925781
48 9318 1.3361681699752808
49 9318 1.6120879650115967
50 9318 1.4130547046661377
51 9318 1.588042974472046
52 9318 1.6165049076080322
53 9318 2.24349308013916
54 9318 1.6469836235046387
55 9318 1.515905737876892
56 9318 1.7417219877243042
57 9318 1.6566741466522217
58 9318 1.5151809453964233
59 9318 1.6203709840774536
60 9318 1.6057705879211426
61 9318 1.292496919631958
62 9318 1.3725616931915283
63 9318 1.6052812337875366
64 9318 1.5573862791061401
65 9318 1.485023021697998
66 9318 1.5843756198883057
67 9318 1.4596455097198486
68 9318 1.736919641494751
69 9318 1.3209153413772583
70 9318 1.5212109088897705
71 9318 1.6043815612792969
72 9318 1.5228973627090454
val - Global loss: 1.568753    Embedding norm: 20.0371   Triplets (all/active): 127.6/127.6
Pos dist (min/mean/max): 0.9139/2.2659/5.9593   Neg dist (min/mean/max): 0.8973/2.3979/6.7316
0 18011 2.655277729034424
1 18011 1.5641106367111206
2 18011 1.7170343399047852
3 18011 5.314916610717773
4 18011 2.4987170696258545
5 18011 2.1839852333068848
6 18011 1.1520216464996338
7 18011 1.0627890825271606
8 18011 1.987365961074829
9 18011 1.8152366876602173
10 18011 2.8630990982055664
11 18011 1.3336882591247559
12 18011 3.0379302501678467
13 18011 11.840377807617188
14 18011 1.0169280767440796
15 18011 1.2866029739379883
16 18011 1.895981788635254
17 18011 1.3509390354156494
18 18011 1.1835495233535767
19 18011 1.170668125152588
20 18011 0.9971432685852051
21 18011 1.351801872253418
22 18011 2.028981924057007
23 18011 7.676433086395264
24 18011 4.670212745666504
25 18011 1.8865748643875122
26 18011 6.518868923187256
27 18011 5.724497318267822
28 18011 4.543721675872803
29 18011 0.7432460784912109
30 18011 1.807809829711914
31 18011 1.806689739227295
32 18011 1.3681795597076416
33 18011 0.48238372802734375
34 18011 0.6118907928466797
35 18011 1.4707081317901611
36 18011 0.6837265491485596
37 18011 0.08806419372558594
38 18011 0.16947698593139648
39 18011 0.8563122749328613
40 18011 0.7635760307312012
Traceback (most recent call last):
  File "/code/RLoc/training/train.py", line 31, in <module>
    do_train(params, debug=args.debug, device=device)
  File "/code/RLoc/training/trainer.py", line 390, in do_train
    torch.cuda.empty_cache()
  File "/usr/local/lib/python3.9/site-packages/torch/cuda/memory.py", line 121, in empty_cache
    torch._C._cuda_emptyCache()
KeyboardInterrupt
