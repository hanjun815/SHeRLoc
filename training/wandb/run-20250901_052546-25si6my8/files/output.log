  0%|                                                    | 0/20 [00:00<?, ?it/s]
0 45360 0.3065721392631531
1 45360 0.37632060050964355
2 45360 0.3564956486225128
3 45360 0.3248855471611023
4 45360 0.3862482011318207
5 45360 0.2617737948894501
6 45360 0.3694496750831604
7 45360 0.31421902775764465
8 45360 0.2309747189283371
9 45360 0.3012993037700653
10 45360 0.375905305147171
11 45360 0.36794668436050415
12 45360 0.45353925228118896
13 45360 0.3419569730758667
14 45360 0.4342426657676697
15 45360 0.3743201792240143
16 45360 0.3443126678466797
17 45360 0.3224504590034485
18 45360 0.3492310345172882
19 45360 0.3831409513950348
20 45360 0.3494803309440613
21 45360 0.3813813328742981
22 45360 0.38165462017059326
23 45360 0.38808903098106384
24 45360 0.3418368101119995
25 45360 0.3658539652824402
26 45360 0.39241963624954224
27 45360 0.35717400908470154
28 45360 0.3481425642967224
29 45360 0.33641716837882996
30 45360 0.4152633249759674
31 45360 0.3849596381187439
32 45360 0.3453388214111328
33 45360 0.3677632510662079
34 45360 0.3426739573478699
35 45360 0.3394959568977356
36 45360 0.3530353605747223
37 45360 0.3384442627429962
38 45360 0.3850296437740326
39 45360 0.33825692534446716
40 45360 0.3140285313129425
41 45360 0.3759525716304779
42 45360 0.37165018916130066
43 45360 0.2474350482225418
44 45360 0.269371896982193
45 45360 0.3129758834838867
46 45360 0.31826546788215637
47 45360 0.30615150928497314
48 45360 0.3393218219280243
49 45360 0.33852922916412354
50 45360 0.31895768642425537
51 45360 0.38221219182014465
52 45360 0.36860889196395874
53 45360 0.3277556896209717
54 45360 0.32981839776039124
55 45360 0.3484233617782593
56 45360 0.32033970952033997
57 45360 0.317515105009079
58 45360 0.3612291216850281
59 45360 0.3419764041900635
60 45360 0.3580333888530731
61 45360 0.3271973729133606
62 45360 0.35463184118270874
63 45360 0.2898816764354706
64 45360 0.29966264963150024
65 45360 0.3111020028591156
66 45360 0.27353695034980774
67 45360 0.3135394752025604
68 45360 0.2507265508174896
69 45360 0.3201174736022949
70 45360 0.3499186038970947
71 45360 0.31876689195632935
72 45360 0.32809552550315857
73 45360 0.31141990423202515
74 45360 0.334322988986969
75 45360 0.34097954630851746
76 45360 0.30365732312202454
77 45360 0.2812515199184418
78 45360 0.27334585785865784
79 45360 0.32341524958610535
80 45360 0.28495484590530396
81 45360 0.2928541600704193
82 45360 0.307472288608551
83 45360 0.30614766478538513
84 45360 0.34671083092689514
85 45360 0.3107594847679138
86 45360 0.30193236470222473
87 45360 0.2844964861869812
88 45360 0.2877157926559448
89 45360 0.2505306005477905
90 45360 0.2567959427833557
91 45360 0.32136353850364685
92 45360 0.28063124418258667
93 45360 0.3176906406879425
94 45360 0.30735328793525696
95 45360 0.2732295095920563
96 45360 0.2672625184059143
97 45360 0.25929391384124756
98 45360 0.30073750019073486
99 45360 0.3044999837875366
100 45360 0.29584887623786926
101 45360 0.2532413899898529
102 45360 0.2361425757408142
103 45360 0.2665456235408783
104 45360 0.26878082752227783
105 45360 0.2581610679626465
106 45360 0.30691617727279663
107 45360 0.2994750738143921
108 45360 0.24030815064907074
109 45360 0.2481498420238495
110 45360 0.293815940618515
111 45360 0.2580929100513458
112 45360 0.3198707699775696
113 45360 0.2530089020729065
114 45360 0.24149569869041443
115 45360 0.22123639285564423
116 45360 0.26877889037132263
117 45360 0.24880315363407135
118 45360 0.3236869275569916
119 45360 0.25912725925445557
120 45360 0.2563379406929016
121 45360 0.2843533456325531
122 45360 0.23373113572597504
123 45360 0.21583066880702972
124 45360 0.20185431838035583
125 45360 0.2118663191795349
126 45360 0.23750749230384827
127 45360 0.2980009615421295
128 45360 0.23210042715072632
129 45360 0.25897544622421265
130 45360 0.2821759283542633
131 45360 0.26198944449424744
132 45360 0.2642136216163635
133 45360 0.25302058458328247
134 45360 0.17829890549182892
135 45360 0.2348722666501999
136 45360 0.214702308177948
137 45360 0.20579585433006287
138 45360 0.18808133900165558
139 45360 0.22906126081943512
140 45360 0.22972114384174347
141 45360 0.4318659007549286
142 45360 0.28038665652275085
143 45360 0.36624687910079956
144 45360 0.37320178747177124
145 45360 0.3787499666213989
146 45360 0.37400826811790466
147 45360 0.38453105092048645
148 45360 0.3723995089530945
149 45360 0.3837483823299408
150 45360 0.29813429713249207
151 45360 0.242126926779747
152 45360 0.280346155166626
153 45360 0.21644718945026398
154 45360 0.23662616312503815
155 45360 0.2544957995414734
156 45360 0.2091464102268219
157 45360 0.2523718774318695
158 45360 0.21501478552818298
159 45360 0.21897532045841217
160 45360 0.21915963292121887
161 45360 0.22257335484027863
162 45360 0.3391839265823364
163 45360 0.3289145529270172
164 45360 0.3136098086833954
165 45360 0.28434813022613525
166 45360 0.2587738037109375
167 45360 0.2494201958179474
168 45360 0.24719293415546417
169 45360 0.2620738744735718
170 45360 0.2802938222885132
171 45360 0.2669900357723236
172 45360 0.3185512125492096
173 45360 0.3193685710430145
174 45360 0.2512451708316803
175 45360 0.18736505508422852
176 45360 0.18365828692913055
177 45360 0.22010284662246704
178 45360 0.23576118052005768
179 45360 0.24520139396190643
180 45360 0.19916555285453796
181 45360 0.23017162084579468
182 45360 0.2848694324493408
183 45360 0.25161314010620117
184 45360 0.2508293390274048
185 45360 0.24757146835327148
186 45360 0.21595324575901031
187 45360 0.30656692385673523
188 45360 0.2927663028240204
189 45360 0.20344002544879913
190 45360 0.24817466735839844
191 45360 0.24206854403018951
192 45360 0.24531610310077667
193 45360 0.2386602908372879
194 45360 0.2150787115097046
195 45360 0.17842166125774384
196 45360 0.1554000973701477
197 45360 0.22475746273994446
198 45360 0.22075727581977844
199 45360 0.21503786742687225
200 45360 0.2679309844970703
201 45360 0.21364463865756989
202 45360 0.22529493272304535
203 45360 0.2253073900938034
204 45360 0.23124316334724426
205 45360 0.21075645089149475
206 45360 0.2204897403717041
207 45360 0.19564245641231537
208 45360 0.20660139620304108
209 45360 0.23384764790534973
210 45360 0.27895939350128174
211 45360 0.17886149883270264
212 45360 0.19946502149105072
213 45360 0.24194583296775818
214 45360 0.2595970034599304
215 45360 0.19675253331661224
216 45360 0.2468264102935791
217 45360 0.24613375961780548
218 45360 0.2344255894422531
219 45360 0.2301175743341446
220 45360 0.24172617495059967
221 45360 0.20213858783245087
222 45360 0.2511852979660034
223 45360 0.21853134036064148
224 45360 0.1852973997592926
225 45360 0.2004644274711609
226 45360 0.21809999644756317
227 45360 0.23575825989246368
228 45360 0.24608734250068665
229 45360 0.19907864928245544
230 45360 0.24831217527389526
231 45360 0.19951222836971283
232 45360 0.21072222292423248
233 45360 0.23881521821022034
234 45360 0.27202072739601135
235 45360 0.2254069447517395
236 45360 0.21044322848320007
237 45360 0.2144857943058014
238 45360 0.20412074029445648
239 45360 0.2258213609457016
240 45360 0.22228308022022247
241 45360 0.2382977306842804
242 45360 0.2392362356185913
243 45360 0.21891865134239197
244 45360 0.1904715597629547
245 45360 0.19394108653068542
246 45360 0.17033834755420685
247 45360 0.18520618975162506
248 45360 0.1525113731622696
249 45360 0.14787553250789642
250 45360 0.1693650186061859
251 45360 0.1642117202281952
train - Global loss: 0.278779    Embedding norm: 1.0000   Triplets (all/active): 180.0/27994.1
Pos dist (min/mean/max): 0.8434/1.2882/1.5766   Neg dist (min/mean/max): 0.9423/1.4086/1.7365
0 1011 0.2736017405986786
1 1011 0.2831353545188904
2 1011 0.2719193994998932
3 1011 0.3087890148162842
4 1011 0.24489237368106842
5 1011 0.323435515165329
val - Global loss: 0.284296    Embedding norm: 1.0000   Triplets (all/active): 168.5/20439.3
Pos dist (min/mean/max): 0.7360/1.1808/1.4985   Neg dist (min/mean/max): 0.7488/1.3061/1.6178
Traceback (most recent call last):
  File "/code/RLoc/training/train.py", line 31, in <module>
    do_train(params, debug=args.debug, device=device)
  File "/code/RLoc/training/trainer.py", line 396, in do_train
    outputs = model(inputs, n_pos, n_neg)
  File "/usr/local/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1130, in _call_impl
    return forward_call(*input, **kwargs)
  File "/usr/local/lib/python3.9/site-packages/torch/nn/parallel/data_parallel.py", line 168, in forward
    outputs = self.parallel_apply(replicas, inputs, kwargs)
  File "/usr/local/lib/python3.9/site-packages/torch/nn/parallel/data_parallel.py", line 178, in parallel_apply
    return parallel_apply(replicas, inputs, kwargs, self.device_ids[:len(replicas)])
  File "/usr/local/lib/python3.9/site-packages/torch/nn/parallel/parallel_apply.py", line 86, in parallel_apply
    output.reraise()
  File "/usr/local/lib/python3.9/site-packages/torch/_utils.py", line 461, in reraise
    raise exception
RuntimeError: Caught RuntimeError in replica 0 on device 0.
Original Traceback (most recent call last):
  File "/usr/local/lib/python3.9/site-packages/torch/nn/parallel/parallel_apply.py", line 61, in _worker
    output = module(*input, **kwargs)
  File "/usr/local/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1130, in _call_impl
    return forward_call(*input, **kwargs)
  File "/code/RLoc/models/model_factory.py", line 214, in forward
    negatives_embedding = self.common_model(negatives)  # Output: [B * n_neg, D]
  File "/usr/local/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1130, in _call_impl
    return forward_call(*input, **kwargs)
  File "/code/RLoc/models/radar/baseline_nets.py", line 65, in forward
    x = self.fe(x)
  File "/usr/local/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1130, in _call_impl
    return forward_call(*input, **kwargs)
  File "/code/RLoc/models/radar/my_vgg.py", line 275, in forward
    return self.feature_extractor(x)
  File "/usr/local/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1130, in _call_impl
    return forward_call(*input, **kwargs)
  File "/usr/local/lib/python3.9/site-packages/torch/nn/modules/container.py", line 139, in forward
    input = module(input)
  File "/usr/local/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1130, in _call_impl
    return forward_call(*input, **kwargs)
  File "/usr/local/lib/python3.9/site-packages/torch/nn/modules/batchnorm.py", line 168, in forward
    return F.batch_norm(
  File "/usr/local/lib/python3.9/site-packages/torch/nn/functional.py", line 2438, in batch_norm
    return torch.batch_norm(
RuntimeError: CUDA out of memory. Tried to allocate 1014.00 MiB (GPU 0; 23.68 GiB total capacity; 4.70 GiB already allocated; 631.88 MiB free; 4.80 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
