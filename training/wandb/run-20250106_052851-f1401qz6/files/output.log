  4%|███▊                                                                                        | 5/120 [21:29<8:11:40, 256.53s/it]Exception in thread Thread-44313:
train - Global loss: 0.224814    Embedding norm: 0.4666   Triplets (all/active): 16.1/16.1
Pos dist (min/mean/max): 0.0000/0.0127/0.0384   Neg dist (min/mean/max): 0.0000/0.0002/0.0037
val - Global loss: 0.200000    Embedding norm: 0.0000   Triplets (all/active): 136.3/136.3
Pos dist (min/mean/max): 0.0000/0.0000/0.0000   Neg dist (min/mean/max): 0.0000/0.0000/0.0000
train - Global loss: 0.200001    Embedding norm: 0.0000   Triplets (all/active): 16.1/16.1
Pos dist (min/mean/max): 0.0000/0.0000/0.0000   Neg dist (min/mean/max): 0.0000/0.0000/0.0000
val - Global loss: 0.200000    Embedding norm: 0.0000   Triplets (all/active): 136.2/136.2
Pos dist (min/mean/max): 0.0000/0.0000/0.0000   Neg dist (min/mean/max): 0.0000/0.0000/0.0000
train - Global loss: 0.200000    Embedding norm: 0.0000   Triplets (all/active): 16.1/16.1
Pos dist (min/mean/max): 0.0000/0.0000/0.0000   Neg dist (min/mean/max): 0.0000/0.0000/0.0000
val - Global loss: 0.200000    Embedding norm: 0.0000   Triplets (all/active): 135.7/135.7
Pos dist (min/mean/max): 0.0000/0.0000/0.0000   Neg dist (min/mean/max): 0.0000/0.0000/0.0000
train - Global loss: 0.200000    Embedding norm: 0.0000   Triplets (all/active): 16.1/16.1
Pos dist (min/mean/max): 0.0000/0.0000/0.0000   Neg dist (min/mean/max): 0.0000/0.0000/0.0000
val - Global loss: 0.200000    Embedding norm: 0.0000   Triplets (all/active): 136.1/136.1
Pos dist (min/mean/max): 0.0000/0.0000/0.0000   Neg dist (min/mean/max): 0.0000/0.0000/0.0000
train - Global loss: 0.200000    Embedding norm: 0.0000   Triplets (all/active): 16.1/16.1
Pos dist (min/mean/max): 0.0000/0.0000/0.0000   Neg dist (min/mean/max): 0.0000/0.0000/0.0000
val - Global loss: 0.200000    Embedding norm: 0.0000   Triplets (all/active): 135.8/135.8
Pos dist (min/mean/max): 0.0000/0.0000/0.0000   Neg dist (min/mean/max): 0.0000/0.0000/0.0000
Traceback (most recent call last):
  File "/usr/local/lib/python3.9/threading.py", line 973, in _bootstrap_inner
    self.run()
  File "/usr/local/lib/python3.9/threading.py", line 910, in run
    self._target(*self._args, **self._kwargs)
  File "/usr/local/lib/python3.9/site-packages/torch/utils/data/_utils/pin_memory.py", line 28, in _pin_memory_loop
    r = in_queue.get(timeout=MP_STATUS_CHECK_INTERVAL)
  File "/usr/local/lib/python3.9/multiprocessing/queues.py", line 122, in get
    return _ForkingPickler.loads(res)
  File "/usr/local/lib/python3.9/site-packages/torch/multiprocessing/reductions.py", line 297, in rebuild_storage_fd
    fd = df.detach()
  File "/usr/local/lib/python3.9/multiprocessing/resource_sharer.py", line 57, in detach
    with _resource_sharer.get_connection(self._id) as conn:
  File "/usr/local/lib/python3.9/multiprocessing/resource_sharer.py", line 86, in get_connection
    c = Client(address, authkey=process.current_process().authkey)
  File "/usr/local/lib/python3.9/multiprocessing/connection.py", line 513, in Client
    answer_challenge(c, authkey)
  File "/usr/local/lib/python3.9/multiprocessing/connection.py", line 757, in answer_challenge
    message = connection.recv_bytes(256)         # reject large message
  File "/usr/local/lib/python3.9/multiprocessing/connection.py", line 221, in recv_bytes
    buf = self._recv_bytes(maxlength)
  File "/usr/local/lib/python3.9/multiprocessing/connection.py", line 419, in _recv_bytes
    buf = self._recv(4)
  File "/usr/local/lib/python3.9/multiprocessing/connection.py", line 384, in _recv
    chunk = read(handle, remaining)
ConnectionResetError: [Errno 104] Connection reset by peer
  4%|███▊                                                                                        | 5/120 [21:31<8:15:15, 258.39s/it]
Traceback (most recent call last):
  File "/code/RLoc/training/train.py", line 31, in <module>
    do_train(params, debug=args.debug, device=device)
  File "/code/RadarLoc/training/trainer.py", line 159, in do_train
    torch.cuda.empty_cache()  # Prevent excessive GPU memory consumption by SparseTensors
  File "/usr/local/lib/python3.9/site-packages/torch/cuda/memory.py", line 121, in empty_cache
    torch._C._cuda_emptyCache()
KeyboardInterrupt
