  0%|                                                   | 0/50 [00:00<?, ?it/s]
0 45360 0.4668858051300049
1 45360 0.48153162002563477
2 45360 0.4662458002567291
3 45360 0.3961765468120575
4 45360 0.38687601685523987
5 45360 0.3755417764186859
6 45360 0.35028916597366333
7 45360 0.5078778862953186
8 45360 0.49887433648109436
9 45360 0.4165087938308716
10 45360 0.5153738260269165
11 45360 0.4884270131587982
12 45360 0.33598417043685913
13 45360 0.3329543173313141
14 45360 0.3203924298286438
15 45360 0.36606815457344055
16 45360 0.38848286867141724
17 45360 0.3777671754360199
18 45360 0.41272011399269104
19 45360 0.4198164939880371
20 45360 0.341105580329895
21 45360 0.4643923342227936
22 45360 0.3662152886390686
23 45360 0.3943893313407898
24 45360 0.36120927333831787
25 45360 0.44377759099006653
26 45360 0.4346347749233246
27 45360 0.39961370825767517
28 45360 0.32891178131103516
29 45360 0.35442832112312317
30 45360 0.33157971501350403
31 45360 0.27139991521835327
32 45360 0.28462183475494385
33 45360 0.308815598487854
34 45360 0.3089827001094818
35 45360 0.30287349224090576
36 45360 0.32514646649360657
37 45360 0.31074029207229614
38 45360 0.3546640872955322
39 45360 0.29935500025749207
40 45360 0.25012943148612976
41 45360 0.29029276967048645
42 45360 0.31181514263153076
43 45360 0.3735745847225189
44 45360 0.3420787751674652
45 45360 0.2354246973991394
46 45360 0.23334677517414093
47 45360 0.29190438985824585
48 45360 0.31533992290496826
49 45360 0.2936301827430725
50 45360 0.3651513159275055
51 45360 0.27695342898368835
52 45360 0.2928304672241211
53 45360 0.2908857762813568
54 45360 0.26385411620140076
55 45360 0.2583560645580292
56 45360 0.20799444615840912
57 45360 0.23731781542301178
58 45360 0.23772278428077698
59 45360 0.2933086156845093
60 45360 0.37826472520828247
61 45360 0.26641979813575745
62 45360 0.22617606818675995
63 45360 0.18543659150600433
64 45360 0.2546512186527252
65 45360 0.2754915654659271
66 45360 0.27558112144470215
67 45360 0.24038347601890564
68 45360 0.25040218234062195
69 45360 0.23786327242851257
70 45360 0.26493796706199646
71 45360 0.2543722093105316
72 45360 0.2213296741247177
73 45360 0.2875717878341675
74 45360 0.19176644086837769
75 45360 0.2114887237548828
76 45360 0.2249017208814621
77 45360 0.2354968637228012
78 45360 0.38539040088653564
79 45360 0.265583872795105
80 45360 0.25058871507644653
81 45360 0.19920165836811066
82 45360 0.2708081901073456
83 45360 0.30699625611305237
84 45360 0.26751792430877686
85 45360 0.25372594594955444
86 45360 0.20168833434581757
87 45360 0.17152990400791168
88 45360 0.1755235195159912
89 45360 0.14885550737380981
90 45360 0.19745323061943054
91 45360 0.23719772696495056
92 45360 0.21501852571964264
93 45360 0.23644055426120758
94 45360 0.1706249713897705
95 45360 0.17734982073307037
96 45360 0.281192421913147
97 45360 0.1808551698923111
98 45360 0.2742618918418884
99 45360 0.5301300883293152
100 45360 0.4579481780529022
101 45360 0.4483785033226013
102 45360 0.4692537784576416
103 45360 0.47496965527534485
104 45360 0.5269332528114319
105 45360 0.4063098728656769
106 45360 0.26203590631484985
107 45360 0.3497622311115265
108 45360 0.3048563003540039
109 45360 0.27060291171073914
110 45360 0.2647438943386078
111 45360 0.18347370624542236
Traceback (most recent call last):
  File "/code/RLoc/training/train.py", line 31, in <module>
    do_train(params, debug=args.debug, device=device)
  File "/code/RLoc/training/trainer.py", line 407, in do_train
    loss.backward()
  File "/usr/local/lib/python3.9/site-packages/torch/_tensor.py", line 396, in backward
    torch.autograd.backward(self, gradient, retain_graph, create_graph, inputs=inputs)
  File "/usr/local/lib/python3.9/site-packages/torch/autograd/__init__.py", line 173, in backward
    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
KeyboardInterrupt
