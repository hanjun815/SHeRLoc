  0%|                                                           | 0/80 [00:00<?, ?it/s]
0 25113 0.37317052483558655
1 25113 0.3365544378757477
2 25113 0.3507228493690491
3 25113 0.3375602066516876
4 25113 0.3265887498855591
5 25113 0.3283589780330658
6 25113 0.31979474425315857
7 25113 0.3197830319404602
8 25113 0.3164525032043457
9 25113 0.31505513191223145
10 25113 0.31646931171417236
11 25113 0.31616300344467163
12 25113 0.31451189517974854
13 25113 0.3121085464954376
14 25113 0.3106303811073303
15 25113 0.3127009868621826
16 25113 0.308948278427124
17 25113 0.31151196360588074
18 25113 0.30943161249160767
19 25113 0.30969172716140747
20 25113 0.309615820646286
21 25113 0.30843281745910645
22 25113 0.30886006355285645
23 25113 0.3051875829696655
24 25113 0.3112832307815552
25 25113 0.30717194080352783
26 25113 0.31434008479118347
27 25113 0.3048136830329895
28 25113 0.3031530976295471
29 25113 0.30390894412994385
30 25113 0.3048924207687378
31 25113 0.30456727743148804
32 25113 0.30202656984329224
33 25113 0.3040904402732849
34 25113 0.3018623888492584
35 25113 0.3041923940181732
36 25113 0.30601218342781067
37 25113 0.30184412002563477
38 25113 0.3013402223587036
39 25113 0.3032532334327698
40 25113 0.3051898777484894
41 25113 0.3015075922012329
42 25113 0.29728108644485474
43 25113 0.30362915992736816
44 25113 0.30153483152389526
45 25113 0.29504138231277466
46 25113 0.29819872975349426
47 25113 0.29930534958839417
48 25113 0.29666638374328613
49 25113 0.30043667554855347
50 25113 0.2996360659599304
51 25113 0.29738089442253113
52 25113 0.29792624711990356
53 25113 0.29557010531425476
54 25113 0.28536826372146606
55 25113 0.2882652282714844
56 25113 0.2886609137058258
57 25113 0.2929409146308899
58 25113 0.27873048186302185
59 25113 0.2771124243736267
60 25113 0.29330721497535706
61 25113 0.2944962978363037
62 25113 0.2539294958114624
63 25113 0.28478723764419556
64 25113 0.2869781255722046
65 25113 0.27741557359695435
66 25113 0.27809080481529236
67 25113 0.2592487931251526
68 25113 0.27869945764541626
69 25113 0.263917475938797
70 25113 0.2851106524467468
71 25113 0.26297202706336975
72 25113 0.2790582776069641
73 25113 0.28490474820137024
74 25113 0.22357484698295593
75 25113 0.26782315969467163
76 25113 0.2188941091299057
77 25113 0.2966368496417999
78 25113 0.27006101608276367
79 25113 0.3402976095676422
80 25113 0.2782963216304779
81 25113 0.2776484787464142
82 25113 0.2870607376098633
83 25113 0.24451731145381927
84 25113 0.2605435252189636
85 25113 0.2859160006046295
86 25113 0.23631024360656738
87 25113 0.266460120677948
88 25113 0.25485342741012573
89 25113 0.24801568686962128
90 25113 0.2160595804452896
91 25113 0.29054343700408936
92 25113 0.18318283557891846
93 25113 0.18997041881084442
94 25113 0.19665853679180145
95 25113 0.22335469722747803
96 25113 0.21134258806705475
97 25113 0.3170439600944519
98 25113 0.21726824343204498
train - Global loss: 0.289320    Embedding norm: 1.0000   Triplets (all/active): 253.7/64536.3
Pos dist (min/mean/max): 0.1170/0.1812/0.3004   Neg dist (min/mean/max): 0.1297/0.2862/0.4831
0 1011 0.3673873543739319
1 1011 0.4233317971229553
2 1011 0.47605130076408386
3 1011 0.5387727618217468
val - Global loss: 0.451386    Embedding norm: 1.0000   Triplets (all/active): 252.8/63728.0
Pos dist (min/mean/max): 0.0743/0.3288/0.9627   Neg dist (min/mean/max): 0.0624/0.4684/1.2829
0 25113 0.25724831223487854
1 25113 0.13121087849140167
2 25113 0.17567330598831177
3 25113 0.23133352398872375
4 25113 0.2315375804901123
5 25113 0.2222546935081482
6 25113 0.2263634353876114
7 25113 0.23385506868362427
8 25113 0.1683931201696396
9 25113 0.24260367453098297
10 25113 0.18922626972198486
11 25113 0.08068111538887024
12 25113 0.28767701983451843
13 25113 0.2000182569026947
14 25113 0.17954875528812408
15 25113 0.286474347114563
16 25113 0.23119789361953735
17 25113 0.2157454639673233
18 25113 0.17285233736038208
19 25113 0.1122080534696579
20 25113 0.09380907565355301
21 25113 0.37429186701774597
22 25113 0.17948812246322632
23 25113 0.17958402633666992
24 25113 0.1213868260383606
25 25113 0.28474441170692444
26 25113 0.290721595287323
27 25113 0.15613707900047302
28 25113 0.22056791186332703
29 25113 0.21718131005764008
30 25113 0.09790042787790298
31 25113 0.29466336965560913
32 25113 0.1765219122171402
33 25113 0.19630944728851318
34 25113 0.21419434249401093
35 25113 0.22467364370822906
36 25113 0.1245872750878334
37 25113 0.17531675100326538
38 25113 0.08236341923475266
39 25113 0.2551632821559906
40 25113 0.08954642713069916
41 25113 0.2429293692111969
42 25113 0.1205974742770195
43 25113 0.10699502378702164
44 25113 0.19900548458099365
45 25113 0.22920763492584229
46 25113 0.25013434886932373
47 25113 0.25167226791381836
48 25113 0.09055429697036743
49 25113 0.08832821995019913
50 25113 0.12679176032543182
51 25113 0.024290448054671288
52 25113 0.3909359574317932
53 25113 0.2635233402252197
54 25113 0.25823524594306946
55 25113 0.06180502474308014
56 25113 0.14585471153259277
57 25113 0.08498900383710861
58 25113 0.14065277576446533
59 25113 0.14931297302246094
60 25113 0.20755280554294586
61 25113 0.4861300587654114
62 25113 0.19894680380821228
63 25113 0.2378232777118683
64 25113 0.40219998359680176
65 25113 0.13153956830501556
66 25113 0.08376608043909073
67 25113 0.00635799765586853
68 25113 0.37983089685440063
69 25113 0.1294916272163391
70 25113 0.3380674123764038
71 25113 0.07059229165315628
72 25113 0.0513983778655529
73 25113 0.11454297602176666
74 25113 0.09975554794073105
75 25113 0.26733148097991943
76 25113 0.028857853263616562
77 25113 0.22168594598770142
78 25113 0.3316965699195862
79 25113 0.4701240360736847
80 25113 0.0
81 25113 0.0
82 25113 0.18311023712158203
83 25113 0.12676197290420532
84 25113 0.010443180799484253
85 25113 0.21085341274738312
86 25113 0.12550823390483856
87 25113 0.14924922585487366
88 25113 0.20965762436389923
89 25113 0.07163992524147034
90 25113 0.04524918645620346
91 25113 0.06539049744606018
92 25113 0.029335230588912964
93 25113 0.17919012904167175
94 25113 0.08008494973182678
95 25113 0.17116078734397888
96 25113 0.06966347992420197
97 25113 0.4605138301849365
98 25113 0.0
train - Global loss: 0.179763    Embedding norm: 1.0000   Triplets (all/active): 253.7/35739.1
Pos dist (min/mean/max): 0.2333/0.4417/0.8130   Neg dist (min/mean/max): 0.4584/0.9828/1.3913
0 1011 0.43449556827545166
1 1011 0.4125881791114807
2 1011 0.43931713700294495
3 1011 0.51036137342453
val - Global loss: 0.449191    Embedding norm: 1.0000   Triplets (all/active): 252.8/63877.8
Pos dist (min/mean/max): 0.2656/0.5197/0.8697   Neg dist (min/mean/max): 0.2239/0.6369/1.0564
0 25113 0.11652164161205292
1 25113 0.04797803610563278
2 25113 0.06726459413766861
3 25113 0.09328372776508331
4 25113 0.4064089059829712
5 25113 0.10613241046667099
6 25113 0.16473092138767242
7 25113 0.06840091943740845
8 25113 0.016495805233716965
9 25113 0.2838892936706543
10 25113 0.12767383456230164
11 25113 0.0
12 25113 0.3410857915878296
13 25113 0.10157691687345505
14 25113 0.09447979927062988
15 25113 0.1607540398836136
16 25113 0.08592668175697327
17 25113 0.052483268082141876
18 25113 0.04621979594230652
19 25113 0.0
20 25113 0.0
21 25113 0.3208903670310974
22 25113 0.0893232524394989
23 25113 0.19287116825580597
24 25113 0.030765384435653687
Traceback (most recent call last):
  File "/usr/local/lib/python3.9/threading.py", line 973, in _bootstrap_inner
    self.run()
  File "/usr/local/lib/python3.9/threading.py", line 910, in run
    self._target(*self._args, **self._kwargs)
  File "/usr/local/lib/python3.9/site-packages/torch/utils/data/_utils/pin_memory.py", line 28, in _pin_memory_loop
    r = in_queue.get(timeout=MP_STATUS_CHECK_INTERVAL)
  File "/usr/local/lib/python3.9/multiprocessing/queues.py", line 122, in get
    return _ForkingPickler.loads(res)
  File "/usr/local/lib/python3.9/site-packages/torch/multiprocessing/reductions.py", line 297, in rebuild_storage_fd
    fd = df.detach()
  File "/usr/local/lib/python3.9/multiprocessing/resource_sharer.py", line 57, in detach
    with _resource_sharer.get_connection(self._id) as conn:
  File "/usr/local/lib/python3.9/multiprocessing/resource_sharer.py", line 86, in get_connection
    c = Client(address, authkey=process.current_process().authkey)
  File "/usr/local/lib/python3.9/multiprocessing/connection.py", line 513, in Client
    answer_challenge(c, authkey)
  File "/usr/local/lib/python3.9/multiprocessing/connection.py", line 762, in answer_challenge
    response = connection.recv_bytes(256)        # reject large message
  File "/usr/local/lib/python3.9/multiprocessing/connection.py", line 221, in recv_bytes
    buf = self._recv_bytes(maxlength)
  File "/usr/local/lib/python3.9/multiprocessing/connection.py", line 419, in _recv_bytes
    buf = self._recv(4)
  File "/usr/local/lib/python3.9/multiprocessing/connection.py", line 388, in _recv
    raise EOFError
EOFError
  2%|█▏                                              | 2/80 [08:58<5:50:10, 269.37s/it]
Traceback (most recent call last):
  File "/code/RLoc/training/train.py", line 31, in <module>
    do_train(params, debug=args.debug, device=device)
  File "/code/RLoc/training/trainer.py", line 386, in do_train
    inputs = torch.cat([query, positives, negatives]).to(device)
KeyboardInterrupt
