  0%|                                                                     | 0/120 [00:00<?, ?it/s]
0 25113 10.699207305908203
1 25113 9.685548782348633
2 25113 9.152522087097168
3 25113 7.457317352294922
4 25113 6.609711647033691
5 25113 4.400030612945557
6 25113 5.0114054679870605
7 25113 2.9833834171295166
8 25113 2.6063153743743896
9 25113 4.564462184906006
10 25113 3.237790107727051
11 25113 2.7197325229644775
12 25113 2.6280765533447266
13 25113 3.112997531890869
14 25113 2.294159173965454
15 25113 2.1004040241241455
16 25113 2.08886981010437
17 25113 2.535984992980957
18 25113 2.2911758422851562
19 25113 2.0970704555511475
20 25113 1.842435359954834
21 25113 1.8666173219680786
22 25113 1.9608454704284668
23 25113 1.720655083656311
24 25113 1.5501770973205566
25 25113 1.797194242477417
26 25113 1.8137646913528442
27 25113 1.748847246170044
28 25113 1.561211109161377
29 25113 1.6394394636154175
30 25113 1.4658421277999878
31 25113 1.6564844846725464
32 25113 1.5345654487609863
33 25113 1.3821148872375488
34 25113 1.3138805627822876
35 25113 1.3541615009307861
36 25113 1.3258991241455078
37 25113 2.1216721534729004
38 25113 1.4902393817901611
39 25113 1.3267689943313599
40 25113 1.5177128314971924
41 25113 1.3944593667984009
42 25113 1.2271085977554321
43 25113 1.435648798942566
44 25113 1.2605794668197632
45 25113 1.471511721611023
46 25113 1.3760098218917847
47 25113 1.2003511190414429
48 25113 1.265066385269165
49 25113 1.312526822090149
50 25113 1.1655629873275757
51 25113 1.5421690940856934
52 25113 1.417641520500183
53 25113 1.2719515562057495
54 25113 1.2360438108444214
55 25113 1.1946406364440918
56 25113 0.9793953895568848
57 25113 1.1415789127349854
58 25113 1.1424449682235718
59 25113 1.2646797895431519
60 25113 1.562544584274292
61 25113 1.2654825448989868
62 25113 1.4206008911132812
63 25113 1.4077171087265015
64 25113 1.4425581693649292
65 25113 1.4701014757156372
66 25113 1.3850979804992676
67 25113 1.3836778402328491
68 25113 1.3262380361557007
69 25113 1.2645890712738037
70 25113 1.3349637985229492
71 25113 1.4464592933654785
72 25113 1.355336308479309
73 25113 1.4195356369018555
74 25113 1.344541072845459
75 25113 1.5646135807037354
76 25113 1.1504100561141968
77 25113 1.2328664064407349
78 25113 1.0855987071990967
79 25113 1.181993842124939
80 25113 1.3150990009307861
81 25113 1.232176661491394
82 25113 1.3371247053146362
83 25113 1.4021695852279663
train - Global loss: 2.129733    Embedding norm: 44.8966   Triplets (all/active): 299.0/79904.0
Pos dist (min/mean/max): 12.4017/15.6274/20.4258   Neg dist (min/mean/max): 11.9510/15.5293/21.5635
0 1011 3.1980018615722656
1 1011 3.679185628890991
2 1011 2.8928327560424805
3 1011 6.247720241546631
val - Global loss: 4.004435    Embedding norm: 43.0261   Triplets (all/active): 252.8/61311.8
Pos dist (min/mean/max): 12.3469/17.2807/25.1180   Neg dist (min/mean/max): 11.3077/16.4037/23.8464
0 25113 0.9502891898155212
1 25113 1.1325055360794067
2 25113 0.9010791778564453
3 25113 1.1189535856246948
4 25113 1.0259500741958618
5 25113 1.0712467432022095
6 25113 1.1375998258590698
7 25113 1.0743237733840942
8 25113 0.9334666728973389
9 25113 1.0795069932937622
10 25113 1.1180431842803955
11 25113 1.0136759281158447
12 25113 1.0471912622451782
13 25113 1.03739595413208
14 25113 0.9041301012039185
15 25113 0.9045477509498596
16 25113 0.8260930776596069
17 25113 0.7872253656387329
18 25113 0.9130664467811584
19 25113 0.9071921110153198
20 25113 0.9255694150924683
21 25113 1.0581974983215332
22 25113 1.0904351472854614
23 25113 0.9144869446754456
24 25113 0.8577895760536194
25 25113 0.9257852435112
26 25113 0.9876735210418701
27 25113 0.9318035244941711
28 25113 0.8086764812469482
29 25113 0.9525777101516724
30 25113 0.8064718246459961
31 25113 0.8943472504615784
32 25113 0.7385388016700745
33 25113 0.8579733967781067
34 25113 0.7535003423690796
35 25113 0.8422617316246033
36 25113 0.686959445476532
37 25113 1.1623473167419434
38 25113 0.8500658273696899
39 25113 0.7032920718193054
40 25113 0.8978504538536072
41 25113 0.7085590958595276
42 25113 0.5966728329658508
43 25113 0.6458207368850708
44 25113 0.643315315246582
45 25113 0.7442528009414673
46 25113 0.7089505791664124
47 25113 0.6849952340126038
48 25113 0.7633170485496521
49 25113 0.824847936630249
50 25113 0.6243823170661926
51 25113 0.7581899166107178
52 25113 0.8676031231880188
53 25113 0.702652096748352
54 25113 0.7242591977119446
55 25113 0.6666669845581055
56 25113 0.6003294587135315
57 25113 0.6295364499092102
58 25113 0.5278278589248657
59 25113 0.6151682138442993
60 25113 0.8279210329055786
61 25113 0.6443333029747009
62 25113 0.8192973732948303
63 25113 0.674896776676178
64 25113 0.6670339107513428
65 25113 0.7947133183479309
66 25113 0.8465707302093506
67 25113 0.7588901519775391
68 25113 0.8986351490020752
69 25113 0.611695408821106
70 25113 0.9136609435081482
71 25113 0.9688950181007385
72 25113 0.7995219230651855
73 25113 0.9287157654762268
74 25113 0.7318160533905029
75 25113 0.9082366824150085
76 25113 0.6060037016868591
77 25113 0.6466730237007141
78 25113 0.6181682348251343
79 25113 0.6412919163703918
80 25113 0.6575592756271362
81 25113 0.7276279926300049
82 25113 0.6942259073257446
83 25113 0.915591299533844
train - Global loss: 0.831779    Embedding norm: 41.3800   Triplets (all/active): 299.0/54949.9
Pos dist (min/mean/max): 11.1847/13.1897/15.8687   Neg dist (min/mean/max): 11.5671/14.3684/17.8374
0 1011 2.4957754611968994
1 1011 3.1415607929229736
2 1011 2.2253329753875732
3 1011 5.352795600891113
val - Global loss: 3.303866    Embedding norm: 39.6338   Triplets (all/active): 252.8/59654.2
Pos dist (min/mean/max): 10.9197/15.3581/22.0577   Neg dist (min/mean/max): 10.0854/15.0717/22.1962
0 25113 0.5215383172035217
1 25113 0.5247681140899658
2 25113 0.5804312229156494
3 25113 0.49202707409858704
4 25113 0.5970104336738586
5 25113 0.7471570372581482
6 25113 0.6894199848175049
7 25113 1.154003620147705
8 25113 0.5044400691986084
9 25113 0.5991465449333191
10 25113 0.7335125207901001
11 25113 0.5157599449157715
12 25113 0.7080847024917603
13 25113 0.6947283148765564
14 25113 0.6005785465240479
15 25113 0.44645896553993225
16 25113 0.5113711357116699
17 25113 0.4755443036556244
18 25113 0.7035969495773315
19 25113 0.5733664631843567
20 25113 0.5721569657325745
21 25113 0.5242689847946167
22 25113 0.9338605403900146
23 25113 0.5413235425949097
24 25113 0.48508399724960327
25 25113 0.6001012325286865
26 25113 0.48338478803634644
27 25113 0.4862736165523529
28 25113 0.6233875155448914
29 25113 0.7063108086585999
30 25113 0.6094213724136353
31 25113 1.0928901433944702
32 25113 0.43044471740722656
33 25113 0.6852229833602905
34 25113 0.36831510066986084
35 25113 0.6197522282600403
36 25113 0.4841754138469696
37 25113 1.0449293851852417
38 25113 0.6581664681434631
39 25113 0.3222300112247467
40 25113 0.6311014294624329
41 25113 0.3900337219238281
42 25113 0.5066919326782227
43 25113 0.4806724488735199
44 25113 0.6549414992332458
45 25113 0.32815849781036377
46 25113 0.30863097310066223
47 25113 0.8712133169174194
48 25113 0.42568454146385193
49 25113 1.0315961837768555
50 25113 0.4946998655796051
51 25113 0.2538263201713562
52 25113 0.7336530685424805
53 25113 0.3327721059322357
54 25113 0.538986325263977
55 25113 0.6258594989776611
56 25113 0.3017119765281677
57 25113 0.4367155432701111
58 25113 0.39790013432502747
59 25113 0.1666823774576187
60 25113 0.45166394114494324
61 25113 0.4545080065727234
62 25113 0.5638899207115173
63 25113 0.34249913692474365
64 25113 0.788131058216095
65 25113 0.3677191436290741
66 25113 0.6658452749252319
67 25113 0.302176296710968
68 25113 0.7643802165985107
69 25113 0.27192482352256775
70 25113 0.5761740207672119
71 25113 0.8688801527023315
72 25113 0.3599776327610016
73 25113 0.7237698435783386
74 25113 0.8338258862495422
75 25113 0.8955789804458618
76 25113 0.42154064774513245
77 25113 0.26845353841781616
78 25113 0.3803819417953491
79 25113 0.27677610516548157
80 25113 0.9204522371292114
81 25113 0.4627842307090759
82 25113 0.36921578645706177
83 25113 0.8560341596603394
train - Global loss: 0.568366    Embedding norm: 39.0167   Triplets (all/active): 299.0/20289.2
Pos dist (min/mean/max): 10.2644/12.1288/14.5356   Neg dist (min/mean/max): 11.5532/14.3723/17.4832
0 1011 2.0977623462677
1 1011 2.567042350769043
2 1011 1.8335063457489014
3 1011 4.389367580413818
val - Global loss: 2.721920    Embedding norm: 37.5781   Triplets (all/active): 252.8/59374.8
Pos dist (min/mean/max): 10.4181/14.3461/20.4444   Neg dist (min/mean/max): 9.9171/14.4733/21.6848
0 25113 0.5433943867683411
1 25113 0.21030272543430328
2 25113 0.6312136650085449
3 25113 0.2924368381500244
4 25113 0.3596237301826477
5 25113 0.6421834230422974
6 25113 0.6025734543800354
7 25113 1.2527858018875122
8 25113 0.3001774251461029
9 25113 0.3881514072418213
10 25113 0.8614976406097412
11 25113 0.3146640360355377
12 25113 0.552297055721283
13 25113 0.5606992840766907
14 25113 0.374462753534317
15 25113 0.25488561391830444
16 25113 0.31956443190574646
17 25113 0.18795251846313477
18 25113 0.6978074312210083
19 25113 0.23236888647079468
20 25113 0.380114883184433
21 25113 0.17312775552272797
22 25113 0.9509245157241821
23 25113 0.36638879776000977
24 25113 0.2883865535259247
25 25113 0.6181501746177673
26 25113 0.27191564440727234
27 25113 0.33539748191833496
28 25113 0.3835616409778595
29 25113 0.7631452083587646
30 25113 0.4891490936279297
31 25113 0.8163459897041321
32 25113 0.09463196992874146
33 25113 0.8266261219978333
34 25113 0.2990805208683014
35 25113 0.37548914551734924
36 25113 0.3860640823841095
37 25113 0.9637913107872009
38 25113 0.609136164188385
39 25113 0.39196163415908813
40 25113 0.43360140919685364
41 25113 0.25610047578811646
42 25113 0.0394660085439682
43 25113 0.19390659034252167
44 25113 0.2756468951702118
45 25113 0.0
46 25113 0.35604172945022583
47 25113 0.7897106409072876
48 25113 0.6532619595527649
49 25113 0.6099017262458801
50 25113 0.29619303345680237
51 25113 0.2757984697818756
52 25113 0.6964425444602966
53 25113 0.5017675757408142
54 25113 0.4147375524044037
55 25113 0.5744743347167969
56 25113 0.25403621792793274
57 25113 0.17711512744426727
58 25113 0.893613874912262
59 25113 0.16022460162639618
60 25113 0.4546893835067749
61 25113 0.293484091758728
62 25113 0.3308262526988983
63 25113 0.10097405314445496
64 25113 0.4535938799381256
65 25113 0.22137919068336487
66 25113 0.8411584496498108
67 25113 0.04969865083694458
68 25113 0.933180034160614
69 25113 0.15351517498493195
70 25113 0.7669433355331421
71 25113 0.4681243896484375
72 25113 0.32465437054634094
73 25113 0.6384188532829285
74 25113 0.48842018842697144
75 25113 0.7544873952865601
76 25113 0.39820462465286255
77 25113 0.27880042791366577
78 25113 0.23255062103271484
79 25113 0.37884581089019775
80 25113 0.5188090801239014
81 25113 0.3421930968761444
82 25113 0.2052089124917984
83 25113 0.6684080958366394
train - Global loss: 0.446560    Embedding norm: 37.8517   Triplets (all/active): 299.0/9762.2
Pos dist (min/mean/max): 9.7588/11.5146/13.7072   Neg dist (min/mean/max): 11.6208/14.3131/17.5060
0 1011 1.8713407516479492
1 1011 2.476135492324829
2 1011 1.6989046335220337
3 1011 4.212823390960693
val - Global loss: 2.564801    Embedding norm: 36.3709   Triplets (all/active): 252.8/58777.5
Pos dist (min/mean/max): 10.0769/13.6707/19.3482   Neg dist (min/mean/max): 9.3284/13.8090/20.1781
0 25113 0.41777554154396057
1 25113 0.0
2 25113 1.2460788488388062
3 25113 0.18664568662643433
4 25113 0.4089774787425995
5 25113 0.6735069751739502
6 25113 0.33674004673957825
7 25113 0.8737940788269043
8 25113 0.31313249468803406
9 25113 0.17983734607696533
10 25113 0.7561620473861694
11 25113 0.4135900139808655
12 25113 0.37278228998184204
13 25113 0.5572748780250549
14 25113 0.35740527510643005
15 25113 0.3178635835647583
16 25113 0.5208200812339783
17 25113 0.0
18 25113 0.5900061726570129
19 25113 0.0
20 25113 0.15962691605091095
21 25113 0.2339266687631607
22 25113 0.9791823029518127
23 25113 0.2366776168346405
24 25113 0.3724420964717865
25 25113 0.5020950436592102
26 25113 0.06384259462356567
27 25113 0.1534946709871292
28 25113 1.2196528911590576
29 25113 0.9302948117256165
30 25113 0.4012089669704437
31 25113 0.5872365236282349
32 25113 0.8992198705673218
33 25113 0.7409576177597046
34 25113 0.35170450806617737
35 25113 0.34418147802352905
36 25113 0.266005277633667
37 25113 1.5227351188659668
38 25113 0.4141961336135864
39 25113 0.8427348732948303
40 25113 0.48085370659828186
41 25113 0.18870002031326294
42 25113 0.3861955404281616
43 25113 0.22500814497470856
44 25113 0.19906209409236908
45 25113 0.162135049700737
46 25113 0.13519901037216187
47 25113 0.7297368049621582
48 25113 0.21728470921516418
49 25113 0.4702353775501251
50 25113 0.27380192279815674
51 25113 0.5482820868492126
52 25113 0.7531469464302063
53 25113 0.18762095272541046
54 25113 0.30754414200782776
55 25113 0.6560203433036804
56 25113 0.18234840035438538
57 25113 0.2223270833492279
58 25113 0.2643056809902191
59 25113 0.255830854177475
60 25113 0.4045356810092926
61 25113 0.29826948046684265
62 25113 0.4432120621204376
63 25113 0.20284706354141235
64 25113 0.3631468713283539
65 25113 0.258148193359375
66 25113 0.6875768303871155
67 25113 0.0
68 25113 0.9428019523620605
69 25113 0.18143656849861145
70 25113 0.5849487781524658
71 25113 0.2811586558818817
72 25113 0.20108658075332642
73 25113 0.6195690631866455
74 25113 0.320781409740448
75 25113 0.7015160322189331
76 25113 0.19745084643363953
77 25113 0.22643254697322845
78 25113 0.3874899744987488
79 25113 0.1256408989429474
80 25113 0.4015372693538666
81 25113 0.2364616096019745
82 25113 0.2537512481212616
83 25113 0.6585059762001038
train - Global loss: 0.423426    Embedding norm: 37.2661   Triplets (all/active): 299.0/7067.8
Pos dist (min/mean/max): 9.3786/11.1560/13.3355   Neg dist (min/mean/max): 11.5596/14.2959/17.4456
0 1011 1.7409679889678955
1 1011 2.2087721824645996
2 1011 1.5820708274841309
3 1011 3.7871315479278564
val - Global loss: 2.329736    Embedding norm: 35.4020   Triplets (all/active): 252.8/57162.2
Pos dist (min/mean/max): 9.8642/13.3615/18.0570   Neg dist (min/mean/max): 9.7537/13.8029/20.5218
0 25113 0.7052880525588989
1 25113 0.0
2 25113 0.9605112075805664
3 25113 0.3349449336528778
4 25113 0.5108116269111633
5 25113 0.6050089597702026
6 25113 0.37805262207984924
7 25113 0.7536771297454834
8 25113 0.5190362334251404
9 25113 0.17126399278640747
10 25113 0.6298861503601074
11 25113 0.3470093309879303
12 25113 0.25217917561531067
13 25113 0.5681416392326355
14 25113 0.5579707026481628
15 25113 0.3661394715309143
16 25113 0.28301379084587097
17 25113 0.0
18 25113 0.6944984197616577
19 25113 0.1482250839471817
20 25113 0.22047165036201477
21 25113 0.22889509797096252
22 25113 0.9121020436286926
23 25113 0.2412072718143463
24 25113 0.465120404958725
25 25113 0.47373369336128235
26 25113 0.42845162749290466
27 25113 0.44582682847976685
28 25113 0.3506263792514801
29 25113 0.5788113474845886
30 25113 0.36324161291122437
31 25113 1.058689832687378
32 25113 0.0
33 25113 0.7820634841918945
34 25113 0.04648667573928833
35 25113 0.3833214044570923
36 25113 0.4171849489212036
37 25113 1.626960039138794
38 25113 0.4663439691066742
39 25113 0.1351097673177719
40 25113 0.462019145488739
41 25113 0.5380241870880127
42 25113 0.0
43 25113 0.0
44 25113 0.2448146939277649
45 25113 0.0
46 25113 0.015974819660186768
47 25113 0.4079348146915436
48 25113 0.13686545193195343
49 25113 0.3387543857097626
50 25113 0.15847688913345337
51 25113 0.14231982827186584
52 25113 0.5394070148468018
53 25113 0.2248639464378357
54 25113 0.2724473774433136
55 25113 0.5345762372016907
56 25113 0.0
57 25113 0.0
58 25113 0.22100204229354858
59 25113 0.0
60 25113 0.33412620425224304
61 25113 0.4583876132965088
62 25113 0.3263932168483734
63 25113 0.18167001008987427
64 25113 0.20857857167720795
65 25113 0.1597680300474167
66 25113 0.5117201805114746
67 25113 0.0
68 25113 0.840032696723938
69 25113 0.17321909964084625
70 25113 0.48491355776786804
71 25113 0.25943511724472046
72 25113 0.22571584582328796
73 25113 0.5715345740318298
74 25113 0.37133583426475525
75 25113 0.6292915344238281
76 25113 0.26090458035469055
77 25113 0.28557974100112915
78 25113 0.3018926978111267
79 25113 0.1722385585308075
80 25113 0.4011352062225342
81 25113 0.20883136987686157
82 25113 0.25806745886802673
83 25113 0.7812333703041077
train - Global loss: 0.369688    Embedding norm: 36.4704   Triplets (all/active): 299.0/6128.2
Pos dist (min/mean/max): 8.9394/10.7076/12.7580   Neg dist (min/mean/max): 11.2796/14.0073/17.0301
0 1011 1.719181776046753
1 1011 2.3357529640197754
2 1011 1.73297917842865
3 1011 4.016928195953369
val - Global loss: 2.451211    Embedding norm: 34.8578   Triplets (all/active): 252.8/58770.2
Pos dist (min/mean/max): 9.7529/13.2189/18.4221   Neg dist (min/mean/max): 9.4223/13.5077/20.4558
0 25113 0.3917590081691742
1 25113 0.0
2 25113 0.46075230836868286
3 25113 0.0
4 25113 0.011411964893341064
5 25113 0.22901371121406555
6 25113 0.2518821358680725
7 25113 0.6169072389602661
8 25113 0.28448349237442017
9 25113 0.5938800573348999
10 25113 0.34869566559791565
11 25113 0.1187257468700409
12 25113 0.1688162237405777
13 25113 0.5387072563171387
14 25113 0.3498650789260864
15 25113 0.028065264225006104
16 25113 0.13436013460159302
17 25113 0.0
18 25113 0.3212587535381317
19 25113 0.21735134720802307
20 25113 0.2653183341026306
21 25113 0.0
22 25113 1.1845604181289673
23 25113 0.20779983699321747
24 25113 0.3443138599395752
25 25113 0.2229948788881302
26 25113 0.17062555253505707
27 25113 0.25043821334838867
Traceback (most recent call last):
  File "/code/RLoc/training/train.py", line 31, in <module>
    do_train(params, debug=args.debug, device=device)
  File "/code/RLoc/training/trainer.py", line 391, in do_train
    outputs = model(inputs, n_pos, n_neg)
  File "/usr/local/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1130, in _call_impl
    return forward_call(*input, **kwargs)
  File "/usr/local/lib/python3.9/site-packages/torch/nn/parallel/data_parallel.py", line 168, in forward
    outputs = self.parallel_apply(replicas, inputs, kwargs)
  File "/usr/local/lib/python3.9/site-packages/torch/nn/parallel/data_parallel.py", line 178, in parallel_apply
    return parallel_apply(replicas, inputs, kwargs, self.device_ids[:len(replicas)])
  File "/usr/local/lib/python3.9/site-packages/torch/nn/parallel/parallel_apply.py", line 78, in parallel_apply
    thread.join()
  File "/usr/local/lib/python3.9/threading.py", line 1053, in join
    self._wait_for_tstate_lock()
  File "/usr/local/lib/python3.9/threading.py", line 1073, in _wait_for_tstate_lock
    if lock.acquire(block, timeout):
KeyboardInterrupt
