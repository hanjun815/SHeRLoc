  0%|                                                    | 0/80 [00:00<?, ?it/s]
0 25113 0.44156306982040405
1 25113 0.596699595451355
2 25113 0.4141797125339508
3 25113 0.48184603452682495
4 25113 0.40984195470809937
5 25113 0.3905857503414154
6 25113 0.5316737294197083
7 25113 0.3847535252571106
8 25113 0.3986941874027252
9 25113 0.43629518151283264
10 25113 0.4834521412849426
11 25113 0.5483867526054382
12 25113 0.48956090211868286
13 25113 0.5358663201332092
14 25113 0.3661060035228729
15 25113 0.39862552285194397
16 25113 0.32386770844459534
17 25113 0.26815053820610046
18 25113 0.35212886333465576
19 25113 0.34685051441192627
20 25113 0.4256703853607178
21 25113 0.3399829864501953
22 25113 0.32828328013420105
23 25113 0.41317614912986755
24 25113 0.38482940196990967
25 25113 0.3061675727367401
26 25113 0.30055156350135803
27 25113 0.34351062774658203
28 25113 0.23814375698566437
29 25113 0.3537125587463379
30 25113 0.3443469703197479
31 25113 0.3334444463253021
32 25113 0.42210203409194946
33 25113 0.33532828092575073
34 25113 0.41653189063072205
35 25113 0.3907186686992645
36 25113 0.3278498649597168
37 25113 0.3233221173286438
38 25113 0.3376515507698059
39 25113 0.3579854965209961
40 25113 0.3362025022506714
41 25113 0.3422505259513855
42 25113 0.31474176049232483
43 25113 0.2599344849586487
44 25113 0.31851303577423096
45 25113 0.3011367619037628
46 25113 0.2828180193901062
47 25113 0.3074173331260681
48 25113 0.3574894666671753
49 25113 0.3140142261981964
50 25113 0.34034934639930725
51 25113 0.24524500966072083
52 25113 0.2973483204841614
53 25113 0.33499836921691895
54 25113 0.24006879329681396
55 25113 0.32989415526390076
56 25113 0.2754555344581604
57 25113 0.2609739303588867
58 25113 0.23817004263401031
59 25113 0.19956432282924652
60 25113 0.19589412212371826
61 25113 0.2373240739107132
62 25113 0.259391188621521
63 25113 0.2165607362985611
64 25113 0.2322852909564972
65 25113 0.20894287526607513
66 25113 0.20655377209186554
67 25113 0.31460410356521606
68 25113 0.30471688508987427
69 25113 0.2966667115688324
70 25113 0.28789231181144714
71 25113 0.3773959279060364
72 25113 0.2375316619873047
73 25113 0.17538557946681976
74 25113 0.35723692178726196
75 25113 0.250211238861084
76 25113 0.2509104907512665
77 25113 0.3124164342880249
78 25113 0.20872269570827484
79 25113 0.2803870439529419
80 25113 0.21541567146778107
81 25113 0.1700848639011383
82 25113 0.19190894067287445
83 25113 0.22325250506401062
84 25113 0.21445949375629425
85 25113 0.20525775849819183
86 25113 0.15292975306510925
87 25113 0.14453966915607452
88 25113 0.23311500251293182
89 25113 0.28879567980766296
90 25113 0.2141052931547165
91 25113 0.19314903020858765
92 25113 0.25230643153190613
93 25113 0.2274780124425888
94 25113 0.21407319605350494
95 25113 0.21247220039367676
96 25113 0.1781342476606369
97 25113 0.21978510916233063
98 25113 0.20906156301498413
99 25113 0.34056007862091064
100 25113 0.28217387199401855
101 25113 0.3209533989429474
102 25113 0.21378223598003387
103 25113 0.20934829115867615
104 25113 0.2198885977268219
105 25113 0.31677600741386414
106 25113 0.3419448733329773
107 25113 0.20359015464782715
108 25113 0.2717463970184326
109 25113 0.29582759737968445
110 25113 0.18744632601737976
111 25113 0.21683822572231293
112 25113 0.15612763166427612
113 25113 0.18377171456813812
114 25113 0.17956316471099854
115 25113 0.17022265493869781
116 25113 0.2774992883205414
117 25113 0.27384868264198303
118 25113 0.23982851207256317
119 25113 0.26661303639411926
120 25113 0.24780435860157013
121 25113 0.1613624542951584
122 25113 0.2393563985824585
123 25113 0.24097652733325958
124 25113 0.20120613276958466
125 25113 0.3123411238193512
train - Global loss: 0.295728    Embedding norm: 1.0000   Triplets (all/active): 199.3/31211.8
Pos dist (min/mean/max): 0.6243/1.0771/1.5259   Neg dist (min/mean/max): 0.7969/1.4037/1.8112
0 1011 0.24452151358127594
1 1011 0.2448960542678833
2 1011 0.24297542870044708
3 1011 0.2772381901741028
4 1011 0.3112936317920685
5 1011 0.23690925538539886
val - Global loss: 0.259639    Embedding norm: 1.0000   Triplets (all/active): 168.5/23123.8
Pos dist (min/mean/max): 0.5325/0.8131/1.1432   Neg dist (min/mean/max): 0.4754/1.1885/1.6182
0 25113 0.28182095289230347
1 25113 0.31124329566955566
2 25113 0.21740172803401947
3 25113 0.23420774936676025
4 25113 0.3574002683162689
5 25113 0.25156036019325256
6 25113 0.19636142253875732
7 25113 0.2715473771095276
8 25113 0.30765146017074585
9 25113 0.21861542761325836
10 25113 0.23593021929264069
11 25113 0.21942250430583954
12 25113 0.28225910663604736
13 25113 0.2318335622549057
14 25113 0.27509522438049316
15 25113 0.20880301296710968
16 25113 0.2720532715320587
17 25113 0.3126831352710724
18 25113 0.1955517828464508
19 25113 0.14569850265979767
20 25113 0.2001379132270813
21 25113 0.22488269209861755
22 25113 0.22955068945884705
23 25113 0.1956983506679535
24 25113 0.22721853852272034
25 25113 0.21124672889709473
Traceback (most recent call last):
  File "/code/RLoc/training/train.py", line 31, in <module>
    do_train(params, debug=args.debug, device=device)
  File "/code/RLoc/training/trainer.py", line 393, in do_train
    outputs = model(inputs, n_pos, n_neg)
  File "/usr/local/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1130, in _call_impl
    return forward_call(*input, **kwargs)
  File "/usr/local/lib/python3.9/site-packages/torch/nn/parallel/data_parallel.py", line 167, in forward
    replicas = self.replicate(self.module, self.device_ids[:len(inputs)])
  File "/usr/local/lib/python3.9/site-packages/torch/nn/parallel/data_parallel.py", line 172, in replicate
    return replicate(module, device_ids, not torch.is_grad_enabled())
  File "/usr/local/lib/python3.9/site-packages/torch/nn/parallel/replicate.py", line 91, in replicate
    param_copies = _broadcast_coalesced_reshape(params, devices, detach)
  File "/usr/local/lib/python3.9/site-packages/torch/nn/parallel/replicate.py", line 71, in _broadcast_coalesced_reshape
    tensor_copies = Broadcast.apply(devices, *tensors)
  File "/usr/local/lib/python3.9/site-packages/torch/nn/parallel/_functions.py", line 23, in forward
    outputs = comm.broadcast_coalesced(inputs, ctx.target_gpus)
  File "/usr/local/lib/python3.9/site-packages/torch/nn/parallel/comm.py", line 58, in broadcast_coalesced
    return torch._C._broadcast_coalesced(tensors, devices, buffer_size)
KeyboardInterrupt
