  0%|                                                                   | 0/80 [00:00<?, ?it/s]
0 25113 2.845289468765259
1 25113 2.3065805435180664
2 25113 3.340761423110962
3 25113 3.5011403560638428
4 25113 4.509341716766357
5 25113 4.935997009277344
6 25113 4.583658695220947
7 25113 5.311607360839844
8 25113 4.711087703704834
9 25113 4.7875142097473145
10 25113 3.1324894428253174
11 25113 3.85892391204834
12 25113 3.327275037765503
13 25113 2.0709450244903564
14 25113 2.614914655685425
15 25113 2.6860344409942627
16 25113 2.535982370376587
17 25113 3.0829172134399414
18 25113 3.0029826164245605
19 25113 3.0010123252868652
20 25113 2.7749733924865723
21 25113 2.685859441757202
22 25113 2.9960403442382812
23 25113 3.66847825050354
24 25113 3.3523716926574707
25 25113 3.419255018234253
26 25113 2.5491855144500732
27 25113 2.4916880130767822
28 25113 2.9805047512054443
29 25113 3.160402774810791
30 25113 2.8995838165283203
31 25113 2.2261741161346436
32 25113 1.5255597829818726
33 25113 1.9979946613311768
34 25113 1.8143539428710938
35 25113 2.008368968963623
36 25113 1.9633774757385254
37 25113 2.965038537979126
38 25113 1.460274338722229
39 25113 2.8939783573150635
40 25113 2.0169551372528076
41 25113 2.6259946823120117
42 25113 1.5914236307144165
43 25113 2.2239487171173096
44 25113 2.2890079021453857
45 25113 1.2758474349975586
46 25113 2.1126232147216797
47 25113 1.8251053094863892
48 25113 1.9474592208862305
49 25113 1.3316601514816284
50 25113 1.6291221380233765
51 25113 2.2581706047058105
52 25113 2.2769598960876465
53 25113 2.0086772441864014
54 25113 1.9614044427871704
55 25113 2.160614490509033
56 25113 1.4888304471969604
57 25113 1.4214098453521729
58 25113 1.633705496788025
59 25113 1.454497218132019
60 25113 2.46376895904541
61 25113 1.4599688053131104
62 25113 1.1681644916534424
63 25113 1.8760831356048584
64 25113 1.8573945760726929
65 25113 1.303197979927063
66 25113 1.1543834209442139
67 25113 1.1629853248596191
68 25113 1.8177350759506226
69 25113 1.7320671081542969
70 25113 1.3112421035766602
71 25113 1.3661171197891235
72 25113 1.2190693616867065
73 25113 1.8503153324127197
74 25113 1.3333498239517212
75 25113 2.015239715576172
76 25113 2.2756083011627197
77 25113 2.056135892868042
78 25113 2.463313341140747
79 25113 2.4229214191436768
80 25113 2.2400095462799072
81 25113 1.2301918268203735
82 25113 1.7329868078231812
83 25113 1.6537983417510986
84 25113 1.3683059215545654
85 25113 2.620871067047119
86 25113 1.2655398845672607
87 25113 1.2308878898620605
88 25113 1.5398093461990356
89 25113 0.94997638463974
90 25113 0.8578961491584778
91 25113 0.9865410327911377
92 25113 0.8941895961761475
93 25113 1.0521970987319946
94 25113 1.2099928855895996
95 25113 1.4740924835205078
96 25113 1.7190096378326416
97 25113 1.5879442691802979
98 25113 1.4987328052520752
train - Global loss: 2.231670    Embedding norm: 12.6577   Triplets (all/active): 253.7/28610.7
Pos dist (min/mean/max): 10.0287/14.9871/22.9085   Neg dist (min/mean/max): 11.5073/18.7680/30.0889
0 1011 1.7698941230773926
1 1011 2.3834738731384277
2 1011 2.516663074493408
3 1011 5.554834365844727
val - Global loss: 3.056216    Embedding norm: 21.4485   Triplets (all/active): 252.8/28119.0
Pos dist (min/mean/max): 5.9674/11.0053/19.8655   Neg dist (min/mean/max): 6.9652/15.4808/31.9842
0 25113 1.7557681798934937
1 25113 0.8232283592224121
2 25113 1.3206340074539185
3 25113 1.4671138525009155
4 25113 1.3892076015472412
5 25113 1.7747883796691895
6 25113 1.37051522731781
7 25113 1.369024395942688
8 25113 0.6418516039848328
9 25113 1.3293466567993164
10 25113 0.7962105870246887
11 25113 1.0300819873809814
12 25113 1.4536722898483276
13 25113 0.7752324938774109
14 25113 1.3467435836791992
15 25113 0.8981342315673828
16 25113 1.3508707284927368
17 25113 0.7871236801147461
18 25113 0.8687359690666199
19 25113 0.8048019409179688
20 25113 0.7301056385040283
21 25113 1.0833897590637207
22 25113 1.2723135948181152
23 25113 0.7401183843612671
24 25113 0.7082619667053223
25 25113 1.129845380783081
26 25113 1.4656962156295776
27 25113 0.7763562202453613
28 25113 1.215412974357605
29 25113 1.0083551406860352
30 25113 0.7281627655029297
31 25113 0.7334733605384827
32 25113 0.5165853500366211
33 25113 0.6399831771850586
34 25113 0.8967291116714478
35 25113 0.7680273056030273
36 25113 0.4875757098197937
37 25113 1.2202860116958618
38 25113 0.5080416202545166
39 25113 0.7885866165161133
40 25113 0.7085952758789062
41 25113 0.7836155295372009
42 25113 0.6751244068145752
43 25113 0.9814913868904114
44 25113 1.2149845361709595
45 25113 0.4799456298351288
46 25113 0.8225835561752319
47 25113 0.6996675133705139
48 25113 0.5939509868621826
49 25113 0.39359256625175476
50 25113 0.7264300584793091
51 25113 0.8038647174835205
52 25113 1.4757013320922852
53 25113 0.8115811944007874
54 25113 0.8869080543518066
55 25113 0.8631559014320374
56 25113 0.6110683679580688
57 25113 0.4077084958553314
58 25113 1.078595757484436
59 25113 0.7208704352378845
60 25113 0.763275146484375
61 25113 0.904464840888977
62 25113 0.6365044116973877
63 25113 0.5820000767707825
64 25113 0.7179645895957947
65 25113 0.6900733113288879
Traceback (most recent call last):
  File "/code/RLoc/training/train.py", line 31, in <module>
    do_train(params, debug=args.debug, device=device)
  File "/code/RLoc/training/trainer.py", line 405, in do_train
    loss.backward()
  File "/usr/local/lib/python3.9/site-packages/torch/_tensor.py", line 396, in backward
    torch.autograd.backward(self, gradient, retain_graph, create_graph, inputs=inputs)
  File "/usr/local/lib/python3.9/site-packages/torch/autograd/__init__.py", line 173, in backward
    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
KeyboardInterrupt
