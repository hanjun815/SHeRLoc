  0%|                                                                                              | 0/120 [00:08<?, ?it/s]
0 11709 0.8130007982254028
1 11709 1.2409828901290894
2 11709 0.7065371870994568
3 11709 0.7498151063919067
4 11709 1.3127427101135254
5 11709 0.9872592091560364
6 11709 2.807965040206909
7 11709 3.1513335704803467
8 11709 1.1443729400634766
9 11709 0.8921928405761719
10 11709 0.7419290542602539
11 11709 0.7102492451667786
12 11709 0.4029809534549713
13 11709 0.6349571347236633
14 11709 0.300057590007782
15 11709 0.1808970421552658
Traceback (most recent call last):
  File "/code/RLoc/training/train.py", line 31, in <module>
    do_train(params, debug=args.debug, device=device)
  File "/code/RLoc/training/trainer.py", line 386, in do_train
    optimizer.step()
  File "/usr/local/lib/python3.9/site-packages/torch/optim/lr_scheduler.py", line 65, in wrapper
    return wrapped(*args, **kwargs)
  File "/usr/local/lib/python3.9/site-packages/torch/optim/optimizer.py", line 113, in wrapper
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.9/site-packages/torch/autograd/grad_mode.py", line 27, in decorate_context
    return func(*args, **kwargs)
  File "/usr/local/lib/python3.9/site-packages/torch/optim/adamw.py", line 161, in step
    adamw(params_with_grad,
  File "/usr/local/lib/python3.9/site-packages/torch/optim/adamw.py", line 218, in adamw
    func(params,
  File "/usr/local/lib/python3.9/site-packages/torch/optim/adamw.py", line 309, in _single_tensor_adamw
    denom = (exp_avg_sq.sqrt() / bias_correction2_sqrt).add_(eps)
KeyboardInterrupt
