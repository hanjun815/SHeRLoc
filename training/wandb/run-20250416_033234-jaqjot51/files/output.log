  0%|                                                    | 0/80 [00:00<?, ?it/s]
0 25113 0.3367878496646881
1 25113 0.32392045855522156
2 25113 0.27770110964775085
3 25113 0.35037896037101746
4 25113 0.30865904688835144
5 25113 0.19441641867160797
6 25113 0.24894654750823975
7 25113 0.16662561893463135
8 25113 0.1680295169353485
9 25113 0.2222413718700409
10 25113 0.2604287564754486
11 25113 0.2844122052192688
12 25113 0.28657034039497375
13 25113 0.301059752702713
14 25113 0.14484329521656036
15 25113 0.18294382095336914
16 25113 0.22887641191482544
17 25113 0.2915470004081726
18 25113 0.2640607953071594
19 25113 0.256855845451355
20 25113 0.29086366295814514
21 25113 0.284968763589859
22 25113 0.3204089403152466
23 25113 0.3153507113456726
24 25113 0.3360476493835449
25 25113 0.34904104471206665
26 25113 0.34319692850112915
27 25113 0.40735337138175964
28 25113 0.29226839542388916
29 25113 0.2762210965156555
30 25113 0.24523942172527313
31 25113 0.15781009197235107
32 25113 0.2466743141412735
33 25113 0.2587895095348358
34 25113 0.23617306351661682
35 25113 0.3193013668060303
36 25113 0.2542959153652191
37 25113 0.3470882475376129
38 25113 0.17713528871536255
39 25113 0.1687493771314621
40 25113 0.22435565292835236
41 25113 0.2695859670639038
42 25113 0.3005056381225586
43 25113 0.27235496044158936
44 25113 0.2959364354610443
45 25113 0.2683543860912323
46 25113 0.3322843611240387
47 25113 0.339385449886322
48 25113 0.33074507117271423
49 25113 0.3828762471675873
50 25113 0.32360127568244934
51 25113 0.29203787446022034
52 25113 0.3201283812522888
53 25113 0.3097170293331146
54 25113 0.422944039106369
55 25113 0.3070976138114929
56 25113 0.302216500043869
57 25113 0.2657792866230011
58 25113 0.3009040057659149
59 25113 0.2605224549770355
60 25113 0.2866785526275635
61 25113 0.32564201951026917
62 25113 0.34184032678604126
63 25113 0.3493792414665222
64 25113 0.3228577673435211
65 25113 0.279837429523468
66 25113 0.307195782661438
67 25113 0.35191190242767334
68 25113 0.30110985040664673
69 25113 0.3819032609462738
70 25113 0.30139678716659546
71 25113 0.338593065738678
72 25113 0.2965645492076874
73 25113 0.27953293919563293
74 25113 0.3149429261684418
75 25113 0.4001297354698181
76 25113 0.298593670129776
77 25113 0.2653282582759857
78 25113 0.2763105034828186
79 25113 0.26113617420196533
80 25113 0.28646111488342285
81 25113 0.28080663084983826
82 25113 0.3099983334541321
83 25113 0.31431344151496887
84 25113 0.3547430634498596
85 25113 0.3053807020187378
86 25113 0.2531757056713104
87 25113 0.2342119812965393
88 25113 0.29320982098579407
89 25113 0.40342462062835693
90 25113 0.3295504152774811
91 25113 0.30642709136009216
92 25113 0.2738581895828247
93 25113 0.31100910902023315
94 25113 0.33247262239456177
95 25113 0.26657602190971375
96 25113 0.35886216163635254
97 25113 0.2617752254009247
98 25113 0.2424757033586502
99 25113 0.29059794545173645
100 25113 0.22036974132061005
101 25113 0.24654754996299744
102 25113 0.1933232843875885
103 25113 0.2056713104248047
104 25113 0.30602142214775085
105 25113 0.23953963816165924
106 25113 0.3652890622615814
107 25113 0.24690595269203186
108 25113 0.25250574946403503
109 25113 0.263104647397995
110 25113 0.1775311529636383
111 25113 0.2228960394859314
112 25113 0.23306898772716522
113 25113 0.1696966588497162
114 25113 0.27632656693458557
115 25113 0.2670023441314697
116 25113 0.3020731508731842
117 25113 0.34371280670166016
118 25113 0.2972075939178467
119 25113 0.3718271255493164
120 25113 0.28952088952064514
121 25113 0.21292416751384735
122 25113 0.3095639944076538
123 25113 0.20746468007564545
124 25113 0.18926823139190674
125 25113 0.2405393272638321
train - Global loss: 0.284030    Embedding norm: 1.0000   Triplets (all/active): 199.3/39620.5
Pos dist (min/mean/max): 0.0043/0.0187/0.0510   Neg dist (min/mean/max): 0.0059/0.0485/0.1042
0 1011 0.15860365331172943
1 1011 0.1909305304288864
2 1011 0.20670939981937408
3 1011 0.1648542881011963
4 1011 0.1629340797662735
5 1011 0.13225829601287842
val - Global loss: 0.169382    Embedding norm: 1.0000   Triplets (all/active): 168.5/33102.8
Pos dist (min/mean/max): 0.0049/0.0223/0.0652   Neg dist (min/mean/max): 0.0030/0.0402/0.1350
0 25113 0.30191296339035034
1 25113 0.2686980962753296
2 25113 0.2401420623064041
3 25113 0.341965913772583
4 25113 0.273084819316864
5 25113 0.18824465572834015
6 25113 0.24618439376354218
7 25113 0.12420612573623657
8 25113 0.13922548294067383
9 25113 0.2046825885772705
10 25113 0.1852617859840393
11 25113 0.2665591835975647
12 25113 0.32014554738998413
13 25113 0.29401737451553345
14 25113 0.07981663197278976
15 25113 0.08910219371318817
16 25113 0.3399254381656647
17 25113 0.33514174818992615
18 25113 0.24108152091503143
19 25113 0.2945477366447449
20 25113 0.30524739623069763
21 25113 0.3016306459903717
22 25113 0.3241157531738281
23 25113 nan
24 25113 nan
25 25113 nan
26 25113 nan
27 25113 nan
28 25113 nan
29 25113 nan
30 25113 nan
31 25113 nan
32 25113 nan
33 25113 nan
Traceback (most recent call last):
  File "/code/RLoc/training/train.py", line 31, in <module>
    do_train(params, debug=args.debug, device=device)
  File "/code/RLoc/training/trainer.py", line 388, in do_train
    inputs = torch.cat([query, positives, negatives]).to(device)
KeyboardInterrupt
