  0%|                                                                                 | 0/120 [00:00<?, ?it/s]
0 22720 3.9516234397888184
1 22720 2.860422372817993
2 22720 3.048353910446167
3 22720 2.8916261196136475
4 22720 2.3785922527313232
5 22720 2.167853832244873
6 22720 2.4209938049316406
7 22720 2.0217134952545166
8 22720 1.6696727275848389
9 22720 1.587112545967102
10 22720 1.3991576433181763
11 22720 1.6532186269760132
12 22720 1.2649481296539307
13 22720 0.9695586562156677
14 22720 0.9860037565231323
15 22720 1.1893219947814941
16 22720 0.8158630132675171
17 22720 0.7173808813095093
18 22720 0.8755122423171997
19 22720 0.7358220815658569
20 22720 0.71048504114151
21 22720 0.8296046257019043
22 22720 0.9456391930580139
23 22720 0.6607441306114197
24 22720 0.6600321531295776
25 22720 0.5630736947059631
26 22720 0.6850142478942871
27 22720 0.6298026442527771
28 22720 0.6722041368484497
29 22720 0.6956573128700256
30 22720 0.511841356754303
31 22720 0.4816576838493347
32 22720 0.5200087428092957
33 22720 0.4925062656402588
34 22720 0.5598915815353394
35 22720 0.5057195425033569
36 22720 0.5000503063201904
37 22720 0.4422937035560608
38 22720 0.3883734345436096
39 22720 0.49645358324050903
40 22720 0.4711434245109558
41 22720 0.4451996088027954
42 22720 0.39295655488967896
43 22720 0.37229204177856445
44 22720 0.4865461587905884
45 22720 0.4125314950942993
46 22720 0.39949655532836914
47 22720 0.47036483883857727
48 22720 0.4654279053211212
49 22720 0.5264954566955566
50 22720 0.4443076252937317
51 22720 0.4813828468322754
52 22720 0.5006878972053528
53 22720 0.39241787791252136
54 22720 0.42160558700561523
55 22720 0.49856022000312805
56 22720 0.4300132989883423
57 22720 0.5164180994033813
58 22720 0.6946044564247131
59 22720 0.4092441499233246
60 22720 0.37192174792289734
61 22720 0.37538981437683105
62 22720 0.40633058547973633
63 22720 0.41642922163009644
64 22720 0.3906874656677246
65 22720 0.3066156208515167
66 22720 0.41310155391693115
67 22720 0.4259459674358368
68 22720 0.35709965229034424
69 22720 0.4236804246902466
70 22720 0.45110562443733215
71 22720 0.49017956852912903
72 22720 0.43416938185691833
73 22720 0.590688169002533
74 22720 0.36857280135154724
75 22720 0.3943576514720917
76 22720 0.5876758694648743
77 22720 0.7642077803611755
78 22720 0.4078904986381531
79 22720 0.4205906093120575
80 22720 0.41285112500190735
81 22720 0.5934069752693176
82 22720 0.28934770822525024
83 22720 0.35131600499153137
84 22720 0.7564964294433594
85 22720 0.2972060739994049
86 22720 0.3290204405784607
87 22720 0.3877483904361725
88 22720 0.23273096978664398
train - Global loss: 0.777913    Embedding norm: 12.7368   Triplets (all/active): 255.3/62819.4
Pos dist (min/mean/max): 1.4841/2.2552/3.4759   Neg dist (min/mean/max): 1.4154/2.3660/5.0822
0 9320 0.45246580243110657
1 9320 0.33824118971824646
2 9320 0.41112110018730164
3 9320 0.4382690191268921
4 9320 0.42767006158828735
5 9320 0.4714284837245941
6 9320 0.5240660905838013
7 9320 0.490562379360199
8 9320 0.5037023425102234
9 9320 0.35342350602149963
10 9320 0.4229615330696106
11 9320 0.3942493200302124
12 9320 0.39347541332244873
13 9320 0.42924514412879944
14 9320 0.41210153698921204
15 9320 0.3637027144432068
16 9320 0.3763269782066345
17 9320 0.47290757298469543
Traceback (most recent call last):
  File "/code/RLoc/training/train.py", line 31, in <module>
    do_train(params, debug=args.debug, device=device)
  File "/code/RLoc/training/trainer.py", line 367, in do_train
    for index, (query, positives, negatives) in enumerate(dataloaders[phase]):
  File "/usr/local/lib/python3.9/site-packages/torch/utils/data/dataloader.py", line 681, in __next__
    data = self._next_data()
  File "/usr/local/lib/python3.9/site-packages/torch/utils/data/dataloader.py", line 721, in _next_data
    data = self._dataset_fetcher.fetch(index)  # may raise StopIteration
  File "/usr/local/lib/python3.9/site-packages/torch/utils/data/_utils/fetch.py", line 49, in fetch
    data = [self.dataset[idx] for idx in possibly_batched_index]
  File "/usr/local/lib/python3.9/site-packages/torch/utils/data/_utils/fetch.py", line 49, in <listcomp>
    data = [self.dataset[idx] for idx in possibly_batched_index]
  File "/code/RLoc/training/dataset.py", line 574, in __getitem__
    #         negatives.append(torch.tensor(neg).unsqueeze(0))
KeyboardInterrupt
