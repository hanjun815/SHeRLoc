  0%|                                                    | 0/80 [00:00<?, ?it/s]
0 25113 0.3631477952003479
1 25113 0.33749920129776
2 25113 0.4027485251426697
3 25113 0.3909913897514343
4 25113 0.37845009565353394
5 25113 0.35098996758461
6 25113 0.29875820875167847
7 25113 0.3522166907787323
8 25113 nan
9 25113 nan
10 25113 nan
11 25113 nan
12 25113 nan
13 25113 nan
14 25113 nan
15 25113 nan
16 25113 nan
17 25113 nan
18 25113 nan
19 25113 nan
20 25113 nan
21 25113 nan
22 25113 nan
23 25113 nan
24 25113 nan
25 25113 nan
26 25113 nan
27 25113 nan
28 25113 nan
29 25113 nan
30 25113 nan
31 25113 nan
32 25113 nan
33 25113 nan
34 25113 nan
35 25113 nan
36 25113 nan
37 25113 nan
38 25113 nan
39 25113 nan
40 25113 nan
41 25113 nan
42 25113 nan
43 25113 nan
44 25113 nan
45 25113 nan
46 25113 nan
47 25113 nan
48 25113 nan
49 25113 nan
50 25113 nan
51 25113 nan
52 25113 nan
53 25113 nan
54 25113 nan
Traceback (most recent call last):
  File "/code/RLoc/training/train.py", line 31, in <module>
    do_train(params, debug=args.debug, device=device)
  File "/code/RLoc/training/trainer.py", line 407, in do_train
    loss.backward()
  File "/usr/local/lib/python3.9/site-packages/torch/_tensor.py", line 396, in backward
    torch.autograd.backward(self, gradient, retain_graph, create_graph, inputs=inputs)
  File "/usr/local/lib/python3.9/site-packages/torch/autograd/__init__.py", line 173, in backward
    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
KeyboardInterrupt
